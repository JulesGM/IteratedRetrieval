{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b516ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 22:58:28,939 [INFO] faiss.loader: Loading faiss with AVX2 support.\n",
      "2021-09-09 22:58:28,940 [INFO] faiss.loader: Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "2021-09-09 22:58:28,941 [INFO] faiss.loader: Loading faiss.\n",
      "2021-09-09 22:58:29,148 [INFO] faiss.loader: Successfully loaded faiss.\n",
      "[139633009272640] 2021-09-09 22:58:33,117 [INFO] transformers.file_utils: PyTorch version 1.7.0 available.\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import logging\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shlex\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# Third party\n",
    "import hydra\n",
    "import jsonlines\n",
    "import rich\n",
    "\n",
    "# Setup Logging\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.getLogger(\"common_retriever\").setLevel(logging.WARNING)\n",
    "\n",
    "# First Party\n",
    "BASE_PATH = Path(\"/home/mila/g/gagnonju/DPR/\")\n",
    "CONF_PATH = BASE_PATH/\"conf\"\n",
    "OUTPUTS = Path(\"/home/mila/g/gagnonju/IteratedDecoding/outputs/\")\n",
    "OUT_FILE = Path(\n",
    "    \"/home/mila/g/gagnonju/DPR/outputs/integrated_script_attempt/\"\n",
    ")\n",
    " \n",
    "os.chdir(BASE_PATH)\n",
    "import common_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03463bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.anaconda3/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'dense_retriever': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/mila/g/gagnonju/.anaconda3/lib/python3.8/site-packages/hydra/core/default_element.py:122: UserWarning: In 'ctx_sources/default_sources': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/mila/g/gagnonju/.anaconda3/lib/python3.8/site-packages/hydra/core/default_element.py:122: UserWarning: In 'datasets/retriever_default': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/mila/g/gagnonju/.anaconda3/lib/python3.8/site-packages/hydra/core/default_element.py:122: UserWarning: In 'encoder/hf_bert': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "[139633009272640] 2021-09-09 22:58:37,120 [INFO] root: args.local_rank -1\n",
      "[139633009272640] 2021-09-09 22:58:37,121 [INFO] root: WORLD_SIZE None\n",
      "[139633009272640] 2021-09-09 22:58:37,298 [INFO] root: Initialized host cn-c005 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "[139633009272640] 2021-09-09 22:58:37,299 [INFO] root: 16-bits training: False \n",
      "[139633009272640] 2021-09-09 22:58:37,311 [INFO] dpr.data.download_data: Requested resource from https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
      "[139633009272640] 2021-09-09 22:58:37,311 [INFO] dpr.data.download_data: Download root_dir /home/mila/g/gagnonju/DPR\n",
      "[139633009272640] 2021-09-09 22:58:37,313 [INFO] dpr.data.download_data: File to be downloaded as /home/mila/g/gagnonju/DPR/downloads/data/wikipedia_split/psgs_w100.tsv\n",
      "[139633009272640] 2021-09-09 22:58:37,314 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/wikipedia_split/psgs_w100.tsv\n",
      "21015325it [04:00, 87249.17it/s]                               \n"
     ]
    }
   ],
   "source": [
    "with hydra.initialize_config_dir(\n",
    "    config_dir=str(CONF_PATH), \n",
    "):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"dense_retriever\",\n",
    "        overrides=[\n",
    "            f\"out_file={shlex.quote(str(OUT_FILE))}\",\n",
    "            f\"batch_size={1024}\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    all_passages, id_prefixes = common_retriever.load_passages(cfg)\n",
    "    n_docs = cfg.n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2915f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[139633009272640] 2021-09-09 23:02:38,604 [INFO] root: args.local_rank -1\n",
      "[139633009272640] 2021-09-09 23:02:38,605 [INFO] root: WORLD_SIZE None\n",
      "[139633009272640] 2021-09-09 23:02:38,606 [INFO] root: Initialized host cn-c005 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "[139633009272640] 2021-09-09 23:02:38,607 [INFO] root: 16-bits training: False \n",
      "[139633009272640] 2021-09-09 23:02:38,610 [INFO] dpr.data.download_data: Requested resource from https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-train.qa.csv\n",
      "[139633009272640] 2021-09-09 23:02:38,611 [INFO] dpr.data.download_data: Download root_dir /home/mila/g/gagnonju/DPR\n",
      "[139633009272640] 2021-09-09 23:02:38,614 [INFO] dpr.data.download_data: File to be downloaded as /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/nq-train.csv\n",
      "[139633009272640] 2021-09-09 23:02:38,615 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/nq-train.csv\n",
      "[139633009272640] 2021-09-09 23:02:38,616 [INFO] dpr.data.download_data: Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
      "[139633009272640] 2021-09-09 23:02:38,617 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/LICENSE\n",
      "[139633009272640] 2021-09-09 23:02:38,618 [INFO] dpr.data.download_data: Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
      "[139633009272640] 2021-09-09 23:02:38,619 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/README\n",
      "[139633009272640] 2021-09-09 23:02:39,494 [INFO] root: args.local_rank -1\n",
      "[139633009272640] 2021-09-09 23:02:39,495 [INFO] root: WORLD_SIZE None\n",
      "[139633009272640] 2021-09-09 23:02:39,496 [INFO] root: Initialized host cn-c005 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "[139633009272640] 2021-09-09 23:02:39,497 [INFO] root: 16-bits training: False \n",
      "[139633009272640] 2021-09-09 23:02:39,499 [INFO] dpr.data.download_data: Requested resource from https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-dev.qa.csv\n",
      "[139633009272640] 2021-09-09 23:02:39,500 [INFO] dpr.data.download_data: Download root_dir /home/mila/g/gagnonju/DPR\n",
      "[139633009272640] 2021-09-09 23:02:39,501 [INFO] dpr.data.download_data: File to be downloaded as /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/nq-dev.csv\n",
      "[139633009272640] 2021-09-09 23:02:39,503 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/nq-dev.csv\n",
      "[139633009272640] 2021-09-09 23:02:39,503 [INFO] dpr.data.download_data: Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
      "[139633009272640] 2021-09-09 23:02:39,505 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/LICENSE\n",
      "[139633009272640] 2021-09-09 23:02:39,506 [INFO] dpr.data.download_data: Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
      "[139633009272640] 2021-09-09 23:02:39,507 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/README\n",
      "[139633009272640] 2021-09-09 23:02:39,791 [INFO] root: args.local_rank -1\n",
      "[139633009272640] 2021-09-09 23:02:39,791 [INFO] root: WORLD_SIZE None\n",
      "[139633009272640] 2021-09-09 23:02:39,792 [INFO] root: Initialized host cn-c005 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "[139633009272640] 2021-09-09 23:02:39,793 [INFO] root: 16-bits training: False \n",
      "[139633009272640] 2021-09-09 23:02:39,796 [INFO] dpr.data.download_data: Requested resource from https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-test.qa.csv\n",
      "[139633009272640] 2021-09-09 23:02:39,797 [INFO] dpr.data.download_data: Download root_dir /home/mila/g/gagnonju/DPR\n",
      "[139633009272640] 2021-09-09 23:02:39,799 [INFO] dpr.data.download_data: File to be downloaded as /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/nq-test.csv\n",
      "[139633009272640] 2021-09-09 23:02:39,801 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/nq-test.csv\n",
      "[139633009272640] 2021-09-09 23:02:39,801 [INFO] dpr.data.download_data: Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
      "[139633009272640] 2021-09-09 23:02:39,802 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/LICENSE\n",
      "[139633009272640] 2021-09-09 23:02:39,803 [INFO] dpr.data.download_data: Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
      "[139633009272640] 2021-09-09 23:02:39,804 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/README\n"
     ]
    }
   ],
   "source": [
    "SETS = [\n",
    "    \"nq_train\", \n",
    "    \"nq_dev\", \n",
    "    \"nq_test\",\n",
    "]\n",
    "\n",
    "with hydra.initialize_config_dir(\n",
    "    config_dir=str(CONF_PATH), \n",
    "):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"dense_retriever\",\n",
    "        overrides=[\n",
    "            f\"out_file={shlex.quote(str(OUT_FILE))}\",\n",
    "            f\"batch_size={1024}\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    datasets = {}\n",
    "    for set_ in SETS:\n",
    "        cfg = hydra.compose(\n",
    "            config_name=\"dense_retriever\",\n",
    "            overrides=[\n",
    "            f\"out_file={shlex.quote(str(OUT_FILE))}\",\n",
    "            f\"qa_dataset={set_}\",\n",
    "        ])\n",
    "        datasets[set_] = common_retriever.load_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9822df78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "print(id_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336ffc3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[139633009272640] 2021-09-09 23:02:40,116 [INFO] root: args.local_rank -1\n",
      "[139633009272640] 2021-09-09 23:02:40,117 [INFO] root: WORLD_SIZE None\n",
      "[139633009272640] 2021-09-09 23:02:40,118 [INFO] root: Initialized host cn-c005 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "[139633009272640] 2021-09-09 23:02:40,119 [INFO] root: 16-bits training: False \n",
      "[139633009272640] 2021-09-09 23:02:40,121 [INFO] root: Reading saved model from /home/mila/g/gagnonju/DPR/dpr/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp\n",
      "[139633009272640] 2021-09-09 23:02:41,630 [INFO] root: model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n",
      "[139633009272640] 2021-09-09 23:02:41,895 [INFO] transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mila/g/gagnonju/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[139633009272640] 2021-09-09 23:02:41,897 [INFO] transformers.configuration_utils: Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[139633009272640] 2021-09-09 23:02:42,077 [INFO] transformers.modeling_utils: loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[139633009272640] 2021-09-09 23:02:45,480 [INFO] transformers.modeling_utils: All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "[139633009272640] 2021-09-09 23:02:45,481 [INFO] transformers.modeling_utils: All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "[139633009272640] 2021-09-09 23:02:45,586 [INFO] transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mila/g/gagnonju/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[139633009272640] 2021-09-09 23:02:45,587 [INFO] transformers.configuration_utils: Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[139633009272640] 2021-09-09 23:02:45,645 [INFO] transformers.modeling_utils: loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[139633009272640] 2021-09-09 23:02:48,897 [INFO] transformers.modeling_utils: All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "[139633009272640] 2021-09-09 23:02:48,898 [INFO] transformers.modeling_utils: All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "[139633009272640] 2021-09-09 23:02:49,002 [INFO] transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/mila/g/gagnonju/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[139633009272640] 2021-09-09 23:03:10,042 [INFO] root: Loading index from /home/mila/g/gagnonju/DPR/dpr/downloads/indexes/single/nq/full\n",
      "[139633009272640] 2021-09-09 23:05:16,140 [INFO] root: Loaded index of type <class 'faiss.swigfaiss.IndexFlat'> and size 21015320\n"
     ]
    }
   ],
   "source": [
    "with hydra.initialize_config_dir(\n",
    "    config_dir=str(CONF_PATH), \n",
    "):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"dense_retriever\",\n",
    "        overrides=[\n",
    "            f\"out_file={shlex.quote(str(OUT_FILE))}\",\n",
    "            f\"batch_size={1024}\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    retriever = common_retriever.make_retriever(cfg, id_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de843212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[139633009272640] 2021-09-09 23:15:03,718 [INFO] __main__: len(datasets[k]) = 3\n",
      "[139633009272640] 2021-09-09 23:15:21,287 [INFO] root: Total encoded queries tensor torch.Size([3610, 768])\n",
      "[139633009272640] 2021-09-09 23:15:21,293 [INFO] __main__: 205.4190261037647 tok/sec for batch size 1048\n",
      "[139633009272640] 2021-09-09 23:15:21,293 [INFO] __main__: Total time: 17.573834656272084\n",
      "[139633009272640] 2021-09-09 23:15:21,294 [INFO] __main__: len(datasets[k]) = 3\n",
      "[139633009272640] 2021-09-09 23:16:04,273 [INFO] root: Total encoded queries tensor torch.Size([8757, 768])\n",
      "[139633009272640] 2021-09-09 23:16:04,284 [INFO] __main__: 203.70176045644735 tok/sec for batch size 1048\n",
      "[139633009272640] 2021-09-09 23:16:04,286 [INFO] __main__: Total time: 42.98931919084862\n",
      "[139633009272640] 2021-09-09 23:16:04,287 [INFO] __main__: len(datasets[k]) = 3\n",
      "[139633009272640] 2021-09-09 23:18:14,454 [INFO] root: Encoded queries 26200\n",
      "[139633009272640] 2021-09-09 23:20:25,349 [INFO] root: Encoded queries 52400\n",
      "[139633009272640] 2021-09-09 23:22:36,239 [INFO] root: Encoded queries 78600\n",
      "[139633009272640] 2021-09-09 23:22:39,351 [INFO] root: Total encoded queries tensor torch.Size([79168, 768])\n",
      "[139633009272640] 2021-09-09 23:22:39,427 [INFO] __main__: 200.35469184522745 tok/sec for batch size 1048\n",
      "[139633009272640] 2021-09-09 23:22:39,428 [INFO] __main__: Total time: 395.1392366751097\n"
     ]
    }
   ],
   "source": [
    "SETS = [\n",
    "    \"nq_test\",\n",
    "    \"nq_dev\",\n",
    "    \"nq_train\",\n",
    "]\n",
    "\n",
    "# This will take a very long time.\n",
    "top_ids_and_scores = {}\n",
    "questions_tensors = {}\n",
    "assert hasattr(retriever, \"batch_size\")\n",
    "retriever.batch_size = 1048 \n",
    "n_docs = 100\n",
    "\n",
    "# Size nq_test: 3610\n",
    "# test, encoding: 17.62 sec with bs=1024\n",
    "# test, encoding: 17.91 sec with bs=2048\n",
    "# train, total 395.1392366751097 sec, bs=1024\n",
    "\n",
    "for k in SETS:\n",
    "    LOGGER.info(f\"Set: {k}\")\n",
    "    LOGGER.info(f\"{len(datasets[k].questions) = }\")\n",
    "    dataset = datasets[k]\n",
    "    start = time.monotonic()\n",
    "    questions_tensors[k] = retriever.generate_question_vectors(\n",
    "        dataset.questions, \n",
    "        query_token=dataset.special_query_token,\n",
    "    )\n",
    "    delta = time.monotonic() - start\n",
    "    \n",
    "    LOGGER.info(f\"{delta / len(dataset.questions)} tok/sec for batch size {retriever.batch_size}\")\n",
    "    LOGGER.info(f\"Total time: {delta}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e4e920a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[139633009272640] 2021-09-09 23:22:39,470 [INFO] __main__: questions_tensors[k].size(0) = 3610\n",
      "[139633009272640] 2021-09-09 23:27:49,206 [INFO] root: index search time: 309.734111 sec.\n",
      "[139633009272640] 2021-09-09 23:27:49,207 [INFO] __main__: 255.59851924341925 tok/sec for n_docs 5\n",
      "[139633009272640] 2021-09-09 23:27:49,208 [INFO] __main__: Total time: 309.73575369035825\n",
      "[139633009272640] 2021-09-09 23:27:49,208 [INFO] __main__: questions_tensors[k].size(0) = 8757\n",
      "[139633009272640] 2021-09-09 23:39:13,167 [INFO] root: index search time: 683.957799 sec.\n",
      "[139633009272640] 2021-09-09 23:39:13,168 [INFO] __main__: 115.74955812513724 tok/sec for n_docs 5\n",
      "[139633009272640] 2021-09-09 23:39:13,169 [INFO] __main__: Total time: 683.9594144662842\n",
      "[139633009272640] 2021-09-09 23:39:13,169 [INFO] __main__: questions_tensors[k].size(0) = 79168\n",
      "[139633009272640] 2021-09-10 01:28:03,513 [INFO] root: index search time: 6530.342558 sec.\n",
      "[139633009272640] 2021-09-10 01:28:03,514 [INFO] __main__: 12.123097666354285 tok/sec for n_docs 5\n",
      "[139633009272640] 2021-09-10 01:28:03,514 [INFO] __main__: Total time: 6530.344156157225\n"
     ]
    }
   ],
   "source": [
    "for k in SETS:\n",
    "    n_docs = 5\n",
    "    # CPU:\n",
    "    #   test, 310 sec for  5 docs\n",
    "    #   train, 6530.344156157225 for 5 docs\n",
    "    LOGGER.info(f\"{questions_tensors[k].size(0) = }\")\n",
    "    start = time.monotonic()\n",
    "    top_ids_and_scores[k] = retriever.get_top_docs(\n",
    "        questions_tensors[k].numpy(), \n",
    "        n_docs,\n",
    "    )\n",
    "    delta = time.monotonic() - start\n",
    "    LOGGER.info(f\"{delta / len(dataset.questions)} sec/tok for n_docs {n_docs}\")\n",
    "    LOGGER.info(f\"Total time: {delta}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2201baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[139633009272640] 2021-09-10 01:28:03,524 [INFO] __main__: /home/mila/g/gagnonju/IteratedDecoding/outputs/nq_retrievals_nq_test.jsonl\n",
      "  0%|          | 0/3610 [00:00<?, ?it/s]\n",
      "[139633009272640] 2021-09-10 01:28:03,585 [INFO] __main__: /home/mila/g/gagnonju/IteratedDecoding/outputs/nq_retrievals_nq_dev.jsonl\n",
      " 87%|████████▋ | 7635/8757 [00:00<00:00, 64964.19it/s]\n",
      "[139633009272640] 2021-09-10 01:28:03,707 [INFO] __main__: /home/mila/g/gagnonju/IteratedDecoding/outputs/nq_retrievals_nq_train.jsonl\n",
      "100%|█████████▉| 78933/79168 [00:00<00:00, 87354.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in SETS:\n",
    "    output_target = OUTPUTS/f\"nq_retrievals_{k}.jsonl\"\n",
    "    LOGGER.info(output_target)\n",
    "    with jsonlines.open(output_target, \"w\") as fout:\n",
    "        for question, answer, (indices, scores) in zip(            \n",
    "            datasets[k].questions, \n",
    "            datasets[k].question_answers, \n",
    "            tqdm.tqdm(top_ids_and_scores[k])\n",
    "        ):\n",
    "            indices = [index for index in indices]\n",
    "            f_scores = [float(score) for score in scores]\n",
    "            fout.write(dict(\n",
    "                question=question, \n",
    "                answer=answer,\n",
    "                indices=indices, \n",
    "                scores=f_scores\n",
    "            ))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10d345ea",
   "metadata": {},
   "source": [
    "OUTPUT_TARGET = /\"train.nq_new_targets\"\n",
    "TOP_K = 10\n",
    "max_i = 50\n",
    "top_ids_and_scores\n",
    "\n",
    "for k in SETS:\n",
    "    with open(OUTPUT_TARGET/f\"{cv_set}_output.target\", \"w\") as fout:\n",
    "        for (i, (question, answer, (indices, scores))\n",
    "            ) in enumerate(zip(\n",
    "            datasets[k].questions, \n",
    "            datasets[k].question_answers,\n",
    "            top_ids_and_scores[k]\n",
    "        )):\n",
    "            \n",
    "            if i > max_i:\n",
    "                break\n",
    "                \n",
    "            indices = [index for index in indices]\n",
    "            f_scores = [float(score) for score in scores]\n",
    "            contexts = [\n",
    "                len(all_passages[index].text.split())\n",
    "                for index in indices[:TOP_K]\n",
    "            ]\n",
    "\n",
    "            rich.print(\n",
    "                f\"{question}:\\n\", \n",
    "                f\"{answer}\\n\", \n",
    "                f\"{len(question.split())}\\n\",\n",
    "                f\"{[len(x.split()) for x in answer]}\\n\",\n",
    "                f\"{contexts}\",\n",
    "            )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21693fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124230d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
