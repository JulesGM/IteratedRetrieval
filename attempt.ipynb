{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d837c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 00:51:13,811 [INFO] faiss.loader: Loading faiss with AVX2 support.\n",
      "2021-09-09 00:51:13,813 [INFO] faiss.loader: Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "2021-09-09 00:51:13,814 [INFO] faiss.loader: Loading faiss.\n",
      "2021-09-09 00:51:14,032 [INFO] faiss.loader: Successfully loaded faiss.\n",
      "[140236006750016] 2021-09-09 00:51:19,861 [INFO] common_retriever: Checking versions...\n",
      "[140236006750016] 2021-09-09 00:51:19,982 [INFO] transformers.file_utils: PyTorch version 1.7.0 available.\n",
      "[140236006750016] 2021-09-09 00:51:24,472 [INFO] common_retriever: All version checks passed.\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shlex\n",
    "\n",
    "# Third party\n",
    "import hydra\n",
    "import rich\n",
    "\n",
    "# First Party\n",
    "BASE_PATH = Path(\"/home/mila/g/gagnonju/DPR/\")\n",
    "CONF_PATH = BASE_PATH/\"conf\"\n",
    "OUTPUTS = Path(\"/home/mila/g/gagnonju/IteratedDecoding/outputs/\")\n",
    "OUT_FILE = Path(\n",
    "    \"/home/mila/g/gagnonju/DPR/outputs/integrated_script_attempt/\"\n",
    ")\n",
    " \n",
    "os.chdir(BASE_PATH)\n",
    "import common_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919bdbe4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.anaconda3/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'dense_retriever': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/mila/g/gagnonju/.anaconda3/lib/python3.8/site-packages/hydra/core/default_element.py:122: UserWarning: In 'ctx_sources/default_sources': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/mila/g/gagnonju/.anaconda3/lib/python3.8/site-packages/hydra/core/default_element.py:122: UserWarning: In 'datasets/retriever_default': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/mila/g/gagnonju/.anaconda3/lib/python3.8/site-packages/hydra/core/default_element.py:122: UserWarning: In 'encoder/hf_bert': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "[140236006750016] 2021-09-09 00:51:24,832 [INFO] root: args.local_rank -1\n",
      "[140236006750016] 2021-09-09 00:51:24,832 [INFO] root: WORLD_SIZE None\n",
      "[140236006750016] 2021-09-09 00:51:25,007 [INFO] root: Initialized host cn-c012 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "[140236006750016] 2021-09-09 00:51:25,009 [INFO] root: 16-bits training: False \n",
      "[140236006750016] 2021-09-09 00:51:25,010 [INFO] root: Reading saved model from /home/mila/g/gagnonju/DPR/dpr/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp\n",
      "[140236006750016] 2021-09-09 00:51:26,269 [INFO] root: model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n",
      "[140236006750016] 2021-09-09 00:51:26,445 [INFO] transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mila/g/gagnonju/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[140236006750016] 2021-09-09 00:51:26,447 [INFO] transformers.configuration_utils: Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[140236006750016] 2021-09-09 00:51:26,565 [INFO] transformers.modeling_utils: loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[140236006750016] 2021-09-09 00:51:29,872 [INFO] transformers.modeling_utils: All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "[140236006750016] 2021-09-09 00:51:29,873 [INFO] transformers.modeling_utils: All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "[140236006750016] 2021-09-09 00:51:30,009 [INFO] transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mila/g/gagnonju/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[140236006750016] 2021-09-09 00:51:30,010 [INFO] transformers.configuration_utils: Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[140236006750016] 2021-09-09 00:51:30,066 [INFO] transformers.modeling_utils: loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[140236006750016] 2021-09-09 00:51:33,308 [INFO] transformers.modeling_utils: All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "[140236006750016] 2021-09-09 00:51:33,309 [INFO] transformers.modeling_utils: All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "[140236006750016] 2021-09-09 00:51:33,401 [INFO] transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/mila/g/gagnonju/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[140236006750016] 2021-09-09 00:51:33,449 [INFO] common_retriever: Selecting standard question encoder\n",
      "[140236006750016] 2021-09-09 00:51:42,021 [INFO] common_retriever: Loading saved model state ...\n",
      "[140236006750016] 2021-09-09 00:51:42,022 [INFO] common_retriever: Encoder state prefix question_model.\n",
      "[140236006750016] 2021-09-09 00:51:42,075 [INFO] common_retriever: Encoder vector_size=768\n",
      "[140236006750016] 2021-09-09 00:51:42,076 [INFO] common_retriever: Loading passages.\n",
      "[140236006750016] 2021-09-09 00:51:42,120 [INFO] dpr.data.download_data: Requested resource from https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
      "[140236006750016] 2021-09-09 00:51:42,121 [INFO] dpr.data.download_data: Download root_dir /home/mila/g/gagnonju/DPR\n",
      "[140236006750016] 2021-09-09 00:51:42,123 [INFO] dpr.data.download_data: File to be downloaded as /home/mila/g/gagnonju/DPR/downloads/data/wikipedia_split/psgs_w100.tsv\n",
      "[140236006750016] 2021-09-09 00:51:42,125 [INFO] dpr.data.download_data: File already exist /home/mila/g/gagnonju/DPR/downloads/data/wikipedia_split/psgs_w100.tsv\n",
      "21015325it [04:11, 83503.80it/s]                              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Done with the reading loop.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mDone with the reading loop.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[140236006750016] 2021-09-09 00:55:53,995 [INFO] common_retriever: Done loading passages.\n",
      "[140236006750016] 2021-09-09 00:55:53,996 [INFO] common_retriever: Loading index.\n",
      "[140236006750016] 2021-09-09 00:55:53,997 [INFO] common_retriever: Index class <class 'dpr.indexer.faiss_indexers.DenseFlatIndexer'> \n",
      "[140236006750016] 2021-09-09 00:55:53,999 [INFO] common_retriever: Done loading index.\n",
      "[140236006750016] 2021-09-09 00:55:54,000 [INFO] common_retriever: Loading retriever.\n",
      "[140236006750016] 2021-09-09 00:55:54,001 [INFO] common_retriever: Loaded retriever.\n",
      "[140236006750016] 2021-09-09 00:55:54,004 [INFO] common_retriever: Index path: /home/mila/g/gagnonju/DPR/dpr/downloads/indexes/single/nq/full\n",
      "[140236006750016] 2021-09-09 00:55:54,004 [INFO] root: Loading index from /home/mila/g/gagnonju/DPR/dpr/downloads/indexes/single/nq/full\n",
      "[140236006750016] 2021-09-09 00:57:17,079 [INFO] root: Loaded index of type <class 'faiss.swigfaiss.IndexFlat'> and size 21015320\n",
      "[140236006750016] 2021-09-09 00:57:18,883 [INFO] common_retriever: ctx_files_patterns: ['/home/mila/g/gagnonju/DPR/downloads/data/wikipedia_split/psgs_w100.tsv']\n",
      "[140236006750016] 2021-09-09 00:57:18,887 [INFO] common_retriever: Index path: /home/mila/g/gagnonju/DPR/dpr/downloads/indexes/single/nq/full\n",
      "[140236006750016] 2021-09-09 00:57:18,887 [INFO] root: Loading index from /home/mila/g/gagnonju/DPR/dpr/downloads/indexes/single/nq/full\n"
     ]
    }
   ],
   "source": [
    "with hydra.initialize_config_dir(\n",
    "    config_dir=str(CONF_PATH), \n",
    "):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"dense_retriever\",\n",
    "        overrides=[\n",
    "        f\"out_file={shlex.quote(str(OUT_FILE))}\",\n",
    "    ])\n",
    "    \n",
    "    retriever, all_passages = common_retriever.make_retriever(cfg)\n",
    "    questions, question_answers, special_query_token = common_retriever.load_data(cfg)\n",
    "    n_docs = cfg.n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ceea68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def retrieve(retriever, all_passages, question, question_answers, special_query_token, n_docs):\n",
    "    if len(all_passages) == 0:\n",
    "        raise RuntimeError(\"No passages data found.\")\n",
    "        \n",
    "    ###########################################################################\n",
    "    # Get top k results.\n",
    "    ###########################################################################\n",
    "    LOGGER.info(\"Using special token %s\", special_query_token)\n",
    "    questions_tensor = retriever.generate_question_vectors(\n",
    "        questions, \n",
    "        query_token=question_answers,\n",
    "    )\n",
    "    \n",
    "    LOGGER.info(f\"get_top_docs: Starting.\")\n",
    "    top_ids_and_scores = retriever.get_top_docs(\n",
    "        questions_tensor.numpy(), \n",
    "        n_docs,\n",
    "    )\n",
    "    LOGGER.info(\"get_top_docs: Done.\")\n",
    "    \n",
    "    return  top_ids_and_scores\n",
    "    \n",
    "top_ids_and_scores = retrieve(retriever, all_passages, question, quetion_answers, special_query_token, n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaff0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCATENATION_TECHNIQUES = dict()\n",
    "MAX_LOOP_N = 15\n",
    "MODEL_PATH = None\n",
    "CONCATENATION_TECHNIQUE = None\n",
    "BATCH_SIZE_QUESTIONS = 10\n",
    "OUTPUT_FILES_ROOT = None\n",
    "\n",
    "make_batches = more_itertools.ichunked\n",
    "\n",
    "\n",
    "def write_contexts(\n",
    "    all_contexts, \n",
    "    context_ids, \n",
    "    output_files_root, \n",
    "    loop_i,\n",
    "):\n",
    "    with open(\n",
    "        output_files_root/f\"contexts_{loop_i}.txt\", \n",
    "        \"a\",\n",
    "    ) as f_out:\n",
    "        for context_id in context_ids:\n",
    "            f_out.write(all_contexts[context_id].strip() + \"\\n\")\n",
    "\n",
    "def write_generations(\n",
    "    generated_text,\n",
    "    output_files_root, \n",
    "    loop_i,\n",
    "):\n",
    "    with open(\n",
    "        output_files_root/f\"generated_{loop_i}.txt\", \n",
    "        \"a\",\n",
    "    ) as f_out:\n",
    "        text = \"\\n\".join((x.strip() for x in generated_text))\n",
    "        f_out.write(text)\n",
    "\n",
    "def inference(\n",
    "    all_contexts,\n",
    "    max_loop_n=MAX_LOOP_N, \n",
    "    model_path=MODEL_PATH, \n",
    "    concatenation_technique=CONCATENATION_TECHNIQUE,\n",
    "    output_files_root=OUTPUT_FILES_ROOT,\n",
    "):\n",
    "    \"\"\"\n",
    "    We currently do \n",
    "    \n",
    "    for batch_questions in questions:\n",
    "        for loop_i in range(max_loop_n)\n",
    "            encode_context_to_gen\n",
    "            (load generator to faster memory)\n",
    "            generate(batch)\n",
    "            decode_text_from_gen\n",
    "            \n",
    "            encode_text_to_retriever\n",
    "            (load retriever to faster memory)\n",
    "            retrieve(batch)\n",
    "            decode_text_from_retriever\n",
    "    \n",
    "    We could do\n",
    "    \n",
    "    for loop_i in range(max_loop_n)\n",
    "        # Parallelize as needed if helpful\n",
    "        # num_questions x num_beams to do\n",
    "        for batch_questions in zip(\n",
    "            retrieved_contexts\n",
    "        ):\n",
    "            encode_contexts_to_gen\n",
    "        \n",
    "        (imaginary barrier)\n",
    "        (load generator to faster memory)    \n",
    "        for batch_questions in questions:\n",
    "            top_beam_ids, top_beam_ppls = generate(batch)\n",
    "            \n",
    "        # Parallelize as needed if helpful\n",
    "        for batch_questions in questions:\n",
    "            decode_text_from_gen\n",
    "            \n",
    "        # Parallelize as needed if helpful\n",
    "        for batch_questions in questions:\n",
    "            encode_text_to_retriever\n",
    "\n",
    "        (imaginary barrier)\n",
    "        # Parallelize as needed if helpful\n",
    "        for batch_questions in questions:\n",
    "            concatenate_contexts\n",
    "\n",
    "        (imaginary barrier)\n",
    "        (load retriever to faster memory)\n",
    "        for batch_questions in questions:            \n",
    "            retrieve(batch)\n",
    "        \n",
    "        # Parallelize as needed if helpful\n",
    "        for batch_questions in questions:            \n",
    "            decode_text_from_retriever\n",
    "    \n",
    "    Much better for GPU memory locality,\n",
    "    worse for total memory use. Would maybe allow\n",
    "    for slightly larger batches.\n",
    "    \n",
    "    I think that the fact that we don't need the \n",
    "    GPU results right away makes it async and faster\n",
    "    \n",
    "    \"\"\"\n",
    "    output_files_root = pathlib.Path(output_files_root)\n",
    "    model = load_model(model_path)\n",
    "    concatenation_technique = CONCATENATION_TECHNIQUES[\n",
    "        concatenation_technique\n",
    "    ]\n",
    "    question_batches = make_batches(questions)\n",
    "    \n",
    "    context_ids_accum = [[] for _ in range(max_loop_n)]\n",
    "    generation_text_accum = [[] for _ in range(max_loop_n)]\n",
    "    \n",
    "    for question_batch in question_batches:\n",
    "        retrieval_query = question\n",
    "        all_generations = []\n",
    "        contexts = retrieve(retrieval_query)\n",
    "\n",
    "        for loop_i in range(max_loop_n):\n",
    "            top_beams_ids, top_beams_ppl = generate(\n",
    "                question, contexts,\n",
    "            )\n",
    "            \n",
    "            # Detokenize top_beams_ids.\n",
    "            # We do this here because we don't have the choice?\n",
    "            # Maybe we do though.\n",
    "            # batch_size x num_beams x (variable num_words)\n",
    "            top_beams_text = []\n",
    "            for unit in top_beams_ids:\n",
    "                top_beams_text.append([\n",
    "                    tokenizer_generator.decode(ids_beam) \n",
    "                    for ids_beam in unit\n",
    "                ])\n",
    "            \n",
    "            # We likely have to decode and re-encode here\n",
    "            retrieval_query = concatenation_technique(\n",
    "                question, \n",
    "                top_beams_text,\n",
    "                top_beams_ppl,\n",
    "                all_results, \n",
    "                loop_i,\n",
    "            )\n",
    "            \n",
    "            context_ids = retrieve(retrieval_query)            \n",
    "            generation_text_accum[loop_i].append(top_beams_text)\n",
    "            context_ids_accum[loop_i].append(contexts)\n",
    "    \n",
    "    \n",
    "    # The following are bad, \n",
    "    context_text_accum = [[] for _ in range(max_loop_n)]\n",
    "\n",
    "    \n",
    "    for i, batch in enumerate(context_ids_accum):\n",
    "        for id_ in batch:\n",
    "            context_text_accum[i].append(all_contexts[id_])\n",
    "    \n",
    "    # [Validate somewhere]\n",
    "    \n",
    "#             write_contexts(\n",
    "#                 all_contexts, \n",
    "#                 context_ids, \n",
    "#                 output_files_root, \n",
    "#                 loop_i,\n",
    "#             )\n",
    "            \n",
    "#             write_generations(\n",
    "#                 generated_text,\n",
    "#                 output_files_root,\n",
    "#                 loop_i,\n",
    "#             )\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
