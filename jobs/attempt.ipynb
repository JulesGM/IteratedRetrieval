{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be40be64-ea72-4329-99ea-790e5b9ef74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29159c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Re)/Loading iterated_utils.py\n",
      "(Re)/Loading iterated_retrieval.py\n",
      "(Re)/Loading common_retriever.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/condaless/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'dense_retriever': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/mila/g/gagnonju/condaless/lib/python3.8/site-packages/hydra/core/default_element.py:122: UserWarning: In 'ctx_sources/default_sources': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/mila/g/gagnonju/condaless/lib/python3.8/site-packages/hydra/core/default_element.py:122: UserWarning: In 'datasets/retriever_default': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/mila/g/gagnonju/condaless/lib/python3.8/site-packages/hydra/core/default_element.py:122: UserWarning: In 'encoder/hf_bert': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:13,191) {iterated_utils.time_this:124}:\n",
      "\u001b[0m\u001b[34mStarting:\u001b[0m Build dataloader\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">data_dir = PosixPath(</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'/home/mila/g/gagnonju/IteratedDecoding/GAR/data/nq-sentence'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mdata_dir = \u001b[0m\u001b[1;31mPosixPath\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m'/home/mila/g/gagnonju/IteratedDecoding/GAR/data/nq-sentence'\u001b[0m\u001b[1;31m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /home/mila/g/gagnonju/IteratedDecoding/GAR/data/nq-sentence/train.target.processed (pkl)... make sure data is what you need\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[INFO] (2021-10-18 18:10:44,664) {iterated_utils.time_this:127}:\n",
      "\u001b[0m\u001b[32mDone:\u001b[0m Build dataloader, 31.47s\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:44,665) {__main__.<module>:214}:\n",
      "\u001b[0mDone.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Imports\n",
    "###############################################################################\n",
    "# Standard library\n",
    "import argparse\n",
    "import collections\n",
    "import contextlib\n",
    "import copy\n",
    "import dataclasses\n",
    "import importlib\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import shlex\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "from typing import *\n",
    "\n",
    "# Third party\n",
    "import beartype\n",
    "import colorama\n",
    "import faiss\n",
    "import hydra\n",
    "import more_itertools\n",
    "import jsonlines\n",
    "import omegaconf\n",
    "import rich\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "# First Party\n",
    "import iterated_utils as utils\n",
    "import iterated_retrieval as ir\n",
    "import iterated_retrieval\n",
    "import common_retriever\n",
    "\n",
    "\n",
    "ROOT_PATH = Path(\"/home/mila/g/gagnonju/IteratedDecoding/\")\n",
    "os.chdir(ROOT_PATH / \"DPR\")\n",
    "import dense_retriever\n",
    "\n",
    "GAR_PATH = ROOT_PATH / \"GAR\" / \"gar\"\n",
    "sys.path.insert(0, str(GAR_PATH))\n",
    "import train_generator\n",
    "import utils_gen\n",
    "assert \"condaless\" in sys.executable, sys.executable\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Logging\n",
    "###############################################################################\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "format_info = (\n",
    "    \"[%(levelname)s] (%(asctime)s) \"\n",
    "    \"{%(name)s.%(funcName)s:%(lineno)d}:\\n\"\n",
    ")\n",
    "\n",
    "logging_format = (\n",
    "    colorama.Fore.CYAN +\n",
    "    format_info +\n",
    "    colorama.Style.RESET_ALL +\n",
    "    \"%(message)s\"\n",
    ")\n",
    "logging.basicConfig(\n",
    "    format=logging_format,\n",
    "    level=logging.INFO,\n",
    "    force=True,\n",
    ")\n",
    "logging.getLogger(\n",
    "    \"transformers.configuration_utils\"\n",
    ").setLevel(logging.WARN)\n",
    "logging.getLogger(\n",
    "    \"transformers.tokenization_utils\"\n",
    ").setLevel(logging.WARN)\n",
    "logging.getLogger(\n",
    "    \"transformers.modeling_utils\"\n",
    ").setLevel(logging.WARN)\n",
    "logging.getLogger(\n",
    "    \"common_retriever\"\n",
    ").setLevel(logging.INFO)\n",
    "logging.getLogger(\n",
    "    \"dense_retriever\"\n",
    ").setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# CONFIG\n",
    "###############################################################################\n",
    "def build_args(root_path):\n",
    "    RUN_NAME = \"first_test\"\n",
    "\n",
    "    SENTENCE_DATA_DIR = root_path / \"GAR/data/nq-sentence\"\n",
    "    SENTENCE_MODEL = root_path / \"GAR/gar/outputs/sentence_with_context/last.ckpt\"\n",
    "\n",
    "    DATA_DIR = SENTENCE_DATA_DIR\n",
    "    DPR_CONF_PATH = ROOT_PATH / \"DPR/conf\"\n",
    "    QUERY_AUG_MODEL_PATH = SENTENCE_MODEL\n",
    "    READER_MODEL_PATH = ROOT_PATH / \"GAR/gar/outputs/answer_with_context/last.ckpt\"\n",
    "\n",
    "    DATALOADER_MAX_TARGET_LEN = 0\n",
    "    DATALOADER_MAX_SOURCE_LEN = 30\n",
    "\n",
    "    GENERATION_BATCH_SIZE = 10\n",
    "    NUM_RETURN_SEQUENCES_QUERY_AUG = 3\n",
    "    RETRIEVER_BATCH_SIZE = 15 // NUM_RETURN_SEQUENCES_QUERY_AUG\n",
    "\n",
    "    AUG_METHOD = \"RETRIEVE_ALL_INDIVIDUALLY\"\n",
    "    MAX_LOOP_N = 15\n",
    "    N_DOCS = 5\n",
    "    MAX_TARGET_LEN = 160\n",
    "    MAX_SOURCE_LEN = 768\n",
    "    FINAL_NUM_CONTEXTS = 5\n",
    "    QUERY_AUG_INPUT_MAX_LEN = 768\n",
    "    DECODING_CONF_QUERY_AUG = iterated_retrieval.DecoderConf(\n",
    "        max_length=MAX_TARGET_LEN,\n",
    "        num_beams=NUM_RETURN_SEQUENCES_QUERY_AUG,\n",
    "        # repetition_penalty=2.5,\n",
    "        # length_penalty=1.0,\n",
    "        temperature=0.5,\n",
    "        num_return_sequences=NUM_RETURN_SEQUENCES_QUERY_AUG,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    DECODING_CONF_READER = iterated_retrieval.DecoderConf(\n",
    "        num_beams=1,\n",
    "        max_length=MAX_TARGET_LEN,\n",
    "        # repetition_penalty=2.5,\n",
    "        # length_penalty=1.0,\n",
    "        num_return_sequences=1,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    OUTPUT_ROOT = ROOT_PATH / \"jobs/iterated_decoding_output/\"\n",
    "    assert OUTPUT_ROOT.exists(), OUTPUT_ROOT\n",
    "\n",
    "    out_path = OUTPUT_ROOT / RUN_NAME\n",
    "    if out_path.exists():\n",
    "        shutil.rmtree(out_path)\n",
    "    out_path.mkdir()\n",
    "\n",
    "    try:\n",
    "        hydra.initialize_config_dir(config_dir=str(DPR_CONF_PATH))\n",
    "    except ValueError as err:\n",
    "        message = (\n",
    "            \"GlobalHydra is already initialized, call \"\n",
    "            \"GlobalHydra.instance().clear() if you want to re-initialize\"\n",
    "        )\n",
    "        if message not in err.args[0]:\n",
    "            raise err\n",
    "\n",
    "\n",
    "    dpr_cfg = hydra.compose(\n",
    "        config_name=\"dense_retriever\",\n",
    "        overrides=[\"out_file=/tmp/\"],\n",
    "    )\n",
    "\n",
    "    args = dict(\n",
    "        conf_path=DPR_CONF_PATH,\n",
    "        data_dir=DATA_DIR,\n",
    "        query_aug_model_path=QUERY_AUG_MODEL_PATH,\n",
    "        reader_model_path=READER_MODEL_PATH,\n",
    "        dataloader_max_target_len=DATALOADER_MAX_TARGET_LEN,\n",
    "        dataloader_max_source_len=DATALOADER_MAX_SOURCE_LEN,\n",
    "        generation_batch_size=GENERATION_BATCH_SIZE,\n",
    "        max_loop_n=MAX_LOOP_N,\n",
    "        n_docs=N_DOCS,\n",
    "        max_source_len=MAX_SOURCE_LEN,\n",
    "        max_target_len=MAX_TARGET_LEN,\n",
    "        query_aug_input_max_len=QUERY_AUG_INPUT_MAX_LEN,\n",
    "        decoding_conf_reader=DECODING_CONF_READER,\n",
    "        decoding_conf_query_aug=DECODING_CONF_QUERY_AUG,\n",
    "        out_path=out_path,\n",
    "        retriever_batch_size=RETRIEVER_BATCH_SIZE,\n",
    "        aug_method=AUG_METHOD,\n",
    "        final_num_contexts=FINAL_NUM_CONTEXTS,\n",
    "    )\n",
    "\n",
    "    json_output_config = dict(\n",
    "        indent=2,\n",
    "        default=utils.json_default,\n",
    "        sort_keys=True,\n",
    "    )\n",
    "\n",
    "    utils.save_json(\n",
    "        args,\n",
    "        out_path / \"args.json\",\n",
    "        **json_output_config\n",
    "    )\n",
    "    utils.save_json(\n",
    "        omegaconf.OmegaConf.to_container(dpr_cfg),\n",
    "        out_path / \"config.json\",\n",
    "        **json_output_config\n",
    "    )\n",
    "\n",
    "    return argparse.Namespace(**args), dpr_cfg\n",
    "\n",
    "\n",
    "args, dpr_cfg = build_args(ROOT_PATH)\n",
    "\n",
    "(dataloader, tokenizer_bart, tokenizer_bert,\n",
    ") = iterated_retrieval.build_tokenizers_and_datasets(\n",
    "    generation_batch_size=args.generation_batch_size,\n",
    "    data_dir=args.data_dir,\n",
    "    max_target_len=args.dataloader_max_target_len,\n",
    "    max_source_len=args.dataloader_max_source_len,\n",
    ")\n",
    "\n",
    "\n",
    "LOGGER.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9afb759",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[INFO] (2021-10-18 18:10:47,723) {iterated_utils.time_this:124}:\n",
      "\u001b[0m\u001b[34mStarting:\u001b[0m common_retriever.load_passages (~6 min)\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:47,778) {dpr.options.setup_cfg_gpu:70}:\n",
      "\u001b[0margs.local_rank -1\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:47,779) {dpr.options.setup_cfg_gpu:73}:\n",
      "\u001b[0mWORLD_SIZE None\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:47,780) {dpr.options.setup_cfg_gpu:89}:\n",
      "\u001b[0mInitialized host cn-d002 as d.rank -1 on device=cuda, n_gpu=4, world size=1\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:47,780) {dpr.options.setup_cfg_gpu:97}:\n",
      "\u001b[0m16-bits training: False \n",
      "\u001b[36m[INFO] (2021-10-18 18:10:47,794) {dpr.data.download_data.download_resource:412}:\n",
      "\u001b[0mRequested resource from https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:47,794) {dpr.data.download_data.download_resource:424}:\n",
      "\u001b[0mDownload root_dir /home/mila/g/gagnonju/IteratedDecoding/DPR\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:47,798) {dpr.data.download_data.download_resource:435}:\n",
      "\u001b[0mFile to be downloaded as /home/mila/g/gagnonju/IteratedDecoding/DPR/downloads/data/wikipedia_split/psgs_w100.tsv\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:47,800) {dpr.data.download_data.download_resource:438}:\n",
      "\u001b[0mFile already exist /home/mila/g/gagnonju/IteratedDecoding/DPR/downloads/data/wikipedia_split/psgs_w100.tsv\n",
      "\u001b[36m[INFO] (2021-10-18 18:10:47,800) {dpr.data.retriever_data.load_data_to:262}:\n",
      "\u001b[0mStarted load_data_to.CsvCtxSrc\n",
      "21015325it [06:14, 56132.84it/s]                              \n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,191) {dpr.data.retriever_data.load_data_to:289}:\n",
      "\u001b[0mDone with load_data_to.CsvCtxSrc\n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,191) {common_retriever.load_passages:175}:\n",
      "\u001b[0mDone loading passages.\n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,192) {iterated_utils.time_this:127}:\n",
      "\u001b[0m\u001b[32mDone:\u001b[0m common_retriever.load_passages (~6 min), 374.47s\n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,192) {iterated_utils.time_this:124}:\n",
      "\u001b[0m\u001b[34mStarting:\u001b[0m common_retriever.make_retriever (~11 min.)\n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,225) {dpr.options.setup_cfg_gpu:70}:\n",
      "\u001b[0margs.local_rank -1\n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,226) {dpr.options.setup_cfg_gpu:73}:\n",
      "\u001b[0mWORLD_SIZE None\n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,226) {dpr.options.setup_cfg_gpu:89}:\n",
      "\u001b[0mInitialized host cn-d002 as d.rank -1 on device=cuda, n_gpu=4, world size=1\n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,227) {dpr.options.setup_cfg_gpu:97}:\n",
      "\u001b[0m16-bits training: False \n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,227) {common_retriever.make_retriever:189}:\n",
      "\u001b[0mLoading encoders.\n",
      "\u001b[36m[INFO] (2021-10-18 18:17:02,228) {dpr.utils.model_utils.load_states_from_checkpoint:169}:\n",
      "\u001b[0mReading saved model from /home/mila/g/gagnonju/IteratedDecoding/DPR/dpr/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp\n",
      "\u001b[36m[INFO] (2021-10-18 18:17:10,513) {dpr.utils.model_utils.load_states_from_checkpoint:173}:\n",
      "\u001b[0mmodel_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_200378/284879130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m retriever, all_passages, special_query_token = common_retriever.build_retriever(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdpr_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m~/IteratedDecoding/jobs/common_retriever.py\u001b[0m in \u001b[0;36mbuild_retriever\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_this\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"common_retriever.make_retriever (~11 min.)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         retriever = (\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mmake_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_prefixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         )\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/IteratedDecoding/jobs/common_retriever.py\u001b[0m in \u001b[0;36mmake_retriever\u001b[0;34m(cfg, id_prefixes)\u001b[0m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     tensorizer, encoder, _ = dense_retriever.init_biencoder_components(\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_model_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     )\n",
      "\u001b[0;32m~/IteratedDecoding/DPR/dpr/models/__init__.py\u001b[0m in \u001b[0;36minit_biencoder_components\u001b[0;34m(encoder_type, args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_biencoder_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minit_comp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBIENCODER_INITIALIZERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/IteratedDecoding/DPR/dpr/models/__init__.py\u001b[0m in \u001b[0;36minit_comp\u001b[0;34m(initializers_dict, type, args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_comp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minitializers_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minitializers_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unsupported model type: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/IteratedDecoding/DPR/dpr/models/__init__.py\u001b[0m in \u001b[0;36minit_hf_bert_biencoder\u001b[0;34m(args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transformers\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please install transformers lib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhf_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_bert_biencoder_components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_bert_biencoder_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/IteratedDecoding/DPR/dpr/models/hf_models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_bert'"
     ]
    }
   ],
   "source": [
    "retriever, all_passages, special_query_token = common_retriever.build_retriever(\n",
    "    dpr_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b072319-2dee-4df3-b8b8-a3be38bc9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.index.index = common_retriever.faiss_to_gpu(retriever.index.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbde20-f004-48cc-af20-40efbc61801e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_aug_model, reader_model = ir.build_models(\n",
    "    reader_model_path=args.reader_model_path,\n",
    "    query_aug_model_path=args.query_aug_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59095362-39ea-45dd-bc2b-bbef7f087e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Specific to selection technique\n",
    "###############################################################################\n",
    "\n",
    "def topk_w_torch(stuff_np: np.ndarray, k, dim):\n",
    "    stuff_pt = torch.Tensor(stuff_np)\n",
    "    try:\n",
    "        end = torch.topk(stuff_pt, k=k, dim=dim).indices.numpy()\n",
    "    except RuntimeError as err:\n",
    "        raise utils.add_to_err(\n",
    "            err,\n",
    "            f\"{stuff_pt.shape = }\\n\"\n",
    "            f\"{dim = }\\n\"\n",
    "            f\"{k = }\\n\"\n",
    "        )\n",
    "\n",
    "    return end\n",
    "\n",
    "\n",
    "def top_k_sum(\n",
    "    scores: np.ndarray, indices: np.ndarray, final_qty: int\n",
    "):\n",
    "    utils.check_shape(scores.shape, indices.shape)\n",
    "    utils.check_equal(scores.ndim, 3)\n",
    "    utils.check_equal(indices.ndim, 3)\n",
    "\n",
    "    output = []\n",
    "    # TODO: inner loops in pure python\n",
    "    for batch_i in range(len(scores)):\n",
    "        per_id = collections.defaultdict(int)\n",
    "        for query_i in range(len(scores[batch_i])):\n",
    "            for retrieved_i in range(len(scores[batch_i][query_i])):\n",
    "                index = indices[batch_i][query_i][retrieved_i]\n",
    "                per_id[index] += scores[batch_i][query_i][retrieved_i]\n",
    "\n",
    "        # TODO: sort the whole list when we don't need to\n",
    "        top_k = sorted(\n",
    "            per_id.items(), key=lambda key_value: -key_value[1]\n",
    "        )[-final_qty:]\n",
    "\n",
    "        top_k_keys = list(zip(*top_k))[0]\n",
    "        output.append(top_k_keys)\n",
    "\n",
    "    utils.check_equal(len(output), scores.shape[0])\n",
    "    output = np.asarray(output)\n",
    "    utils.check_equal(output.shape[0], scores.shape[0])\n",
    "    return output\n",
    "\n",
    "\n",
    "def topk_w_numpy(stuff_np: np.ndarray, k, dim):\n",
    "    indices = np.argpartition(\n",
    "        stuff_np, -k, axis=dim\n",
    "    )\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_reference(arr, indices):\n",
    "    assert arr is not None\n",
    "    assert indices is not None\n",
    "    assert arr.shape[0] == indices.shape[0], (arr.shape[0], indices.shape[0])\n",
    "    for batch_i in range(arr.shape[0]):\n",
    "        arr[batch_i] = arr[batch_i, indices[batch_i]]\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def get_torch(arr, indices):\n",
    "    return torch.gather(\n",
    "        input=torch.Tensor(arr), index=torch.Tensor(indices).long(), dim=1\n",
    "    ).data.numpy()\n",
    "\n",
    "\n",
    "def get_numpy(arr, indices):\n",
    "    return np.take_along_axis(arr, indices, 1)\n",
    "\n",
    "\n",
    "@utils.class_checker\n",
    "@dataclasses.dataclass\n",
    "class SelectionTechniqueChecksInfo:\n",
    "    batch_size: int\n",
    "    num_sequences: int\n",
    "    n_docs: int\n",
    "    loop_i: int\n",
    "\n",
    "\n",
    "@beartype.beartype\n",
    "def selection_technique(\n",
    "    top_ids_np: np.ndarray,\n",
    "    scores_retr_np: np.ndarray,\n",
    "    final_num_contexts: int,\n",
    "    query_scores_batch: np.ndarray,\n",
    "    checks_info: SelectionTechniqueChecksInfo,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    DUMB_TOP_K = \"DUMB_TOP_K\"\n",
    "    # UNIQUE_DUMB_TOP_K = \"UNIQUE_DUMB_TOP_K\"\n",
    "    ADDITIVE_TOP_K = \"ADDITIVE_TOP_K\"\n",
    "    MARGINAL = \"MARGINAL_TOP_K\"\n",
    "    mode = ADDITIVE_TOP_K\n",
    "\n",
    "    # Shape verifications\n",
    "    utils.check_shape(top_ids_np.shape, (\n",
    "         checks_info.batch_size, checks_info.num_sequences, checks_info.n_docs\n",
    "    ))\n",
    "    effective_batch_size, queries_per_question, n_docs = top_ids_np.shape\n",
    "\n",
    "    if mode == DUMB_TOP_K:\n",
    "        # Actual Work\n",
    "        top_ids_np = top_ids_np.reshape(\n",
    "            effective_batch_size,  queries_per_question * n_docs\n",
    "        )\n",
    "        scores_retr_np = scores_retr_np.reshape(\n",
    "            effective_batch_size,  queries_per_question * n_docs\n",
    "        )\n",
    "\n",
    "        indices_w_torch = topk_w_torch(\n",
    "            scores_retr_np, final_num_contexts, dim=1,\n",
    "        )\n",
    "        assert indices_w_torch is not None\n",
    "        output = get_reference(top_ids_np, indices_w_torch)\n",
    "\n",
    "    elif mode == ADDITIVE_TOP_K:\n",
    "        output = top_k_sum(\n",
    "            scores=scores_retr_np,\n",
    "            indices=top_ids_np,\n",
    "            final_qty=final_num_contexts,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"mode {mode} not implemented or invalid\")\n",
    "\n",
    "    # Shape Verification\n",
    "    try:\n",
    "        utils.check_shape(\n",
    "            output.shape, (checks_info.batch_size, final_num_contexts)\n",
    "        )\n",
    "\n",
    "    except ValueError as err:\n",
    "        raise utils.add_to_err(\n",
    "            err,\n",
    "            f\"\\t- {checks_info.batch_size = }\\n\"\n",
    "            f\"\\t- {checks_info.num_sequences = }\\n\"\n",
    "            f\"\\t- {checks_info.n_docs = }\\n\"\n",
    "            f\"\\t- {checks_info.loop_i = }\\n\"\n",
    "        )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Inference\n",
    "###############################################################################\n",
    "@beartype.beartype\n",
    "def inference(\n",
    "    all_passages: Dict[str, str],\n",
    "    query_aug_model: train_generator.SummarizationTrainer,\n",
    "    reader_model: train_generator.SummarizationTrainer,\n",
    "    special_query_token: Optional[str],\n",
    "    retriever: dense_retriever.LocalFaissRetriever,\n",
    "    selection_technique: Callable,\n",
    "    question_dataloader: torch.utils.data.DataLoader,\n",
    "    max_loop_n: int,\n",
    "    decoding_conf_reader: ir.DecoderConf,\n",
    "    decoding_conf_query_aug: ir.DecoderConf,\n",
    "    query_aug_input_max_length: int,\n",
    "    n_docs: int,\n",
    "    out_path: Union[str, Path],\n",
    "    retriever_batch_size: int,\n",
    "    aug_method: str,\n",
    "    final_num_contexts: int,\n",
    "    generation_batch_size: int,\n",
    ") -> None:\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "\n",
    "    # Prepare the output files\n",
    "    prefixes = dict(\n",
    "        retr_outs=\"retr_outs_\",\n",
    "        reader_outs=\"reader_outs_\",\n",
    "        q_aug_outs=\"q_aug_outs_\",\n",
    "        gen_inputs=\"gen_inputs_\",\n",
    "        retr_inputs=\"retr_inputs_\",\n",
    "    )\n",
    "\n",
    "    for prefix in prefixes.values():\n",
    "        for path in out_path.glob(f\"{prefix}*.jsonl\"):\n",
    "            LOGGER.info(f\"Deleting path: {path}\")\n",
    "            os.remove(path)\n",
    "\n",
    "    with torch.inference_mode(True):\n",
    "        query_aug_text_all_loops = []\n",
    "        query_aug_score_all_loops = []\n",
    "\n",
    "        for loop_i in range(max_loop_n):\n",
    "            LOGGER.info(f\"{loop_i = }\")\n",
    "            output_paths = {}\n",
    "\n",
    "            for name, prefix in prefixes.items():\n",
    "                output_paths[name] = out_path / f\"{prefix}{loop_i}.jsonl\"\n",
    "\n",
    "            ###################################################################\n",
    "            # PREPARE THE RETRIEVAL QUERIES\n",
    "            ###################################################################\n",
    "            all_queries_this_loop = []\n",
    "            all_queries_scores_this_loop = []\n",
    "            questions_batching_generator = ir.question_generator(\n",
    "                question_dataloader,\n",
    "                tokenizer_bart,\n",
    "                f\"[{loop_i = }] Preparing the retrieval queries :: \",\n",
    "            )\n",
    "\n",
    "            if loop_i == 0:\n",
    "                query_batch_generator = (\n",
    "                    None for _ in range(len(question_dataloader))\n",
    "                )\n",
    "                query_batch_scores_generator = (\n",
    "                    None for _ in range(len(question_dataloader))\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                query_batch_generator = more_itertools.chunked(\n",
    "                    query_aug_text_all_loops[-1],\n",
    "                    question_dataloader.batch_size,\n",
    "                )\n",
    "                query_batch_scores_generator = more_itertools.chunked(\n",
    "                    query_aug_score_all_loops[-1],\n",
    "                    question_dataloader.batch_size,\n",
    "                )\n",
    "\n",
    "\n",
    "            for batch_i, (\n",
    "                questions_batch, query_aug_batch, query_aug_batch_scores\n",
    "            ) in enumerate(\n",
    "                more_itertools.zip_equal(\n",
    "                    questions_batching_generator,\n",
    "                    query_batch_generator,\n",
    "                    query_batch_scores_generator,\n",
    "                )\n",
    "            ):\n",
    "\n",
    "                if loop_i == 0:\n",
    "                    assert query_aug_batch is None\n",
    "                    # The questions are our queries.\n",
    "                    all_queries_this_loop.extend(\n",
    "                        [[x] for x in questions_batch]\n",
    "                    )\n",
    "                else:\n",
    "                    query_aug_batch = np.array(query_aug_batch, dtype=\"object\")\n",
    "\n",
    "                    # Use the query augs to augment the question.\n",
    "                    if aug_method == \"RETRIEVE_ALL_INDIVIDUALLY\":\n",
    "                        # If we retrieve all queries individually, then\n",
    "                        # we keep the 1:1 relationship between the score\n",
    "                        # qty and the query qty\n",
    "\n",
    "                        utils.check_equal(\n",
    "                            query_aug_batch.shape[1],\n",
    "                            decoding_conf_query_aug.num_return_sequences,\n",
    "                        )\n",
    "                        utils.check_equal(query_aug_batch.ndim, 2)\n",
    "                        for i, (question, query_set) in enumerate(\n",
    "                            more_itertools.zip_equal(\n",
    "                                questions_batch,\n",
    "                                query_aug_batch,\n",
    "                            )\n",
    "                        ):\n",
    "                            per_question = []\n",
    "                            for gen in query_set:\n",
    "                                sentence = (\n",
    "                                    question + tokenizer_bert.sep_token + gen\n",
    "                                )\n",
    "                                per_question.append(sentence)\n",
    "\n",
    "                            all_queries_this_loop.append(per_question)\n",
    "                    else:\n",
    "                        raise ValueError(aug_method)\n",
    "\n",
    "            iterated_retrieval.write_generations(\n",
    "                all_queries_this_loop, output_paths[\"retr_inputs\"],\n",
    "            )\n",
    "            \n",
    "\n",
    "            ###################################################################\n",
    "            # RETRIEVE\n",
    "            ###################################################################\n",
    "            retrieved_this_loop = []\n",
    "            with utils.time_this(\"retrieve\", no_start=True):\n",
    "                # If we are at loop_i == 0, the number of queries is 1\n",
    "                # so the number of retrievals is batch_size * 1, which is\n",
    "                # num_augs times smaller than it is for loop_i > 0. To\n",
    "                # compensate, we make the batches larger by a factor of\n",
    "                # num_augs.\n",
    "                if loop_i == 0:\n",
    "                    effective_batch_size = (\n",
    "                        retriever_batch_size *\n",
    "                        decoding_conf_query_aug.num_return_sequences\n",
    "                    )\n",
    "                    queries_per_question = 1\n",
    "                    all_queries_scores_this_loop = [\n",
    "                        None for _ in range(len(all_queries_this_loop))\n",
    "                    ]\n",
    "                else:\n",
    "                    effective_batch_size = retriever_batch_size\n",
    "                    queries_per_question = (\n",
    "                        decoding_conf_query_aug.num_return_sequences\n",
    "                    )\n",
    "                    all_queries_scores_this_loop = (\n",
    "                        query_aug_score_all_loops[-1]\n",
    "                    )\n",
    "\n",
    "                # Make sure we have as many scores as we have queries.\n",
    "                # This should always be true.\n",
    "                try:\n",
    "                    utils.check_equal(\n",
    "                        len(all_queries_this_loop),\n",
    "                        len(all_queries_scores_this_loop),\n",
    "                    )\n",
    "\n",
    "                except ValueError as err:\n",
    "                    raise utils.add_to_err(\n",
    "                        err, (\n",
    "                            f\"{len(all_queries_this_loop) = }\\n\"\n",
    "                            f\"{np.array(all_queries_this_loop).shape = }\\n\"\n",
    "                            f\"{len(all_queries_scores_this_loop) = }\\n\"\n",
    "                            f\"{np.array(all_queries_scores_this_loop).shape = }\\n\"\n",
    "                            f\"{loop_i = }\\n\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                for batch_i, (query_batch, query_scores_batch) in enumerate(\n",
    "                    more_itertools.zip_equal(\n",
    "                        more_itertools.chunked(\n",
    "                            tqdm.notebook.tqdm(\n",
    "                                all_queries_this_loop,\n",
    "                                desc=\"retrieval all_queries_this_loop\",\n",
    "                            ),\n",
    "                            effective_batch_size\n",
    "                        ),\n",
    "                        more_itertools.chunked(\n",
    "                            all_queries_scores_this_loop,\n",
    "                            effective_batch_size,\n",
    "                        ),\n",
    "                    )\n",
    "                ):\n",
    "\n",
    "                    # Retrieve.\n",
    "                    if aug_method == \"RETRIEVE_ALL_INDIVIDUALLY\":\n",
    "                        query_batch_np = np.array(\n",
    "                            query_batch, dtype=\"object\",\n",
    "                        ).reshape(-1)\n",
    "                        \n",
    "                        real_batch_size = len(query_batch)\n",
    "\n",
    "                        # TODO: Make sure the reshaping makes sense\n",
    "                        top_ids_and_scores = common_retriever.retrieve(\n",
    "                            retriever=retriever,\n",
    "                            all_passages=all_passages,\n",
    "                            questions=query_batch_np,\n",
    "                            special_query_token=special_query_token,\n",
    "                            n_docs=n_docs,\n",
    "                        )\n",
    "\n",
    "                        ################################################\n",
    "                        # Deal with contexts\n",
    "                        ################################################\n",
    "                        top_ids, scores_retr = more_itertools.zip_equal(\n",
    "                            *top_ids_and_scores\n",
    "                        )\n",
    "\n",
    "                        try:\n",
    "                            top_ids_np = np.array(top_ids).reshape(\n",
    "                                real_batch_size,\n",
    "                                queries_per_question,\n",
    "                                n_docs\n",
    "                            )\n",
    "                            scores_retr_np = np.array(scores_retr).reshape(\n",
    "                                real_batch_size,\n",
    "                                queries_per_question,\n",
    "                                n_docs,\n",
    "                            )\n",
    "                        except ValueError as err:\n",
    "                            args = utils.add_to_err(\n",
    "                                f\"\\t- {top_ids = }\\n\"\n",
    "                                f\"\\t- {real_batch_size = }\\n\"\n",
    "                                f\"\\t- {effective_batch_size = }\\n\"\n",
    "                                f\"\\t- {queries_per_question = }\\n\"\n",
    "                                f\"\\t- {n_docs = }\\n\"\n",
    "                                f\"\\t- {loop_i = }\"\n",
    "                            )\n",
    "                            raise err\n",
    "\n",
    "                        selected_contexts_ids_np = selection_technique(\n",
    "                            top_ids_np,\n",
    "                            scores_retr_np,\n",
    "                            final_num_contexts,\n",
    "                            np.array(query_scores_batch),\n",
    "                            SelectionTechniqueChecksInfo(\n",
    "                                batch_size=real_batch_size,\n",
    "                                num_sequences=queries_per_question,\n",
    "                                n_docs=n_docs,\n",
    "                                loop_i=loop_i,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        utils.check_shape(\n",
    "                            selected_contexts_ids_np.shape,\n",
    "                            (\n",
    "                                real_batch_size,\n",
    "                                final_num_contexts\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        retrieved_this_loop.extend(selected_contexts_ids_np)\n",
    "\n",
    "                    else:\n",
    "                        raise ValueError(aug_method)\n",
    "\n",
    "                    iterated_retrieval.write_contexts(\n",
    "                        all_contexts=all_passages,\n",
    "                        context_ids=selected_contexts_ids_np.tolist(),\n",
    "                        out_path=output_paths[\"retr_outs\"],\n",
    "                    )\n",
    "\n",
    "            del selected_contexts_ids_np\n",
    "            del all_queries_this_loop\n",
    "            del all_queries_scores_this_loop\n",
    "\n",
    "            ###################################################################\n",
    "            # Generation with the Barts\n",
    "            ###################################################################\n",
    "            LOGGER.info(f\"[{loop_i = }] Starting generation.\")\n",
    "            query_aug_text_all_loops.append([])\n",
    "            query_aug_score_all_loops.append([])\n",
    "\n",
    "            utils.check_equal(len(query_aug_text_all_loops), loop_i + 1)\n",
    "            utils.check_equal(len(query_aug_score_all_loops), loop_i + 1)\n",
    "            tqdm_info = f\"[{loop_i = }] Generating with BART models :: \"\n",
    "            num_batchs_generation = np.ceil(\n",
    "                len(retrieved_this_loop) / question_dataloader.batch_size\n",
    "            )\n",
    "            utils.check_equal(\n",
    "                question_dataloader.batch_size,\n",
    "                generation_batch_size,\n",
    "            )\n",
    "            try:\n",
    "                utils.check_equal(\n",
    "                    num_batchs_generation,\n",
    "                    len(question_dataloader),\n",
    "                )\n",
    "            except ValueError as err:\n",
    "                err = utils.add_to_err(\n",
    "                    f\"\\t- {loop_i = }\\n\"\n",
    "                    , err\n",
    "                )\n",
    "                raise err\n",
    "\n",
    "            for batch_i, (questions_batch, context_batch) in enumerate(\n",
    "                more_itertools.zip_equal(\n",
    "                    iterated_retrieval.question_generator(\n",
    "                        question_dataloader, tokenizer_bart, tqdm_info,\n",
    "                    ),\n",
    "                    more_itertools.chunked(\n",
    "                        retrieved_this_loop, generation_batch_size,\n",
    "                    ),\n",
    "                )\n",
    "            ):\n",
    "                utils.check_equal(len(questions_batch), len(context_batch))\n",
    "\n",
    "                try:\n",
    "                    utils.check_batch_size(\n",
    "                        len(questions_batch),\n",
    "                        generation_batch_size,\n",
    "                        len(question_dataloader.dataset),\n",
    "                    )\n",
    "                    utils.check_batch_size(\n",
    "                        len(context_batch),\n",
    "                        generation_batch_size,\n",
    "                        len(question_dataloader.dataset),\n",
    "                    )\n",
    "                except RuntimeError as err:\n",
    "                    raise utils.add_to_err(\n",
    "                        err,\n",
    "                        f\"\\t- {loop_i = }\\n\"\n",
    "                        f\"\\t- {batch_i = }\\n\"\n",
    "                    )\n",
    "\n",
    "                ###############################################################\n",
    "                # PREPARE GENERATION INPUTS\n",
    "                ###############################################################\n",
    "                # Take the contexts and append them to the questions\n",
    "                gen_inputs_text = []\n",
    "                for question, selected_ids in more_itertools.zip_equal(\n",
    "                    questions_batch,\n",
    "                    context_batch,\n",
    "                ):\n",
    "                    contexts = [\n",
    "                        all_passages[ids_].text\n",
    "                        for ids_ in selected_ids\n",
    "                    ]\n",
    "\n",
    "                    generation_input = (\n",
    "                        question + tokenizer_bart.sep_token +\n",
    "                        tokenizer_bart.sep_token.join(contexts)\n",
    "                    )\n",
    "\n",
    "                    gen_inputs_text.append(generation_input)\n",
    "\n",
    "                utils.check_batch_size(\n",
    "                    len(gen_inputs_text),\n",
    "                    generation_batch_size,\n",
    "                    len(question_dataloader.dataset),\n",
    "                )\n",
    "\n",
    "                gen_inputs = tokenizer_bart.batch_encode_plus(\n",
    "                    gen_inputs_text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    pad_to_max_length=True,\n",
    "                    max_length=query_aug_input_max_length,\n",
    "                )\n",
    "\n",
    "                ###############################################################\n",
    "                # QUERY_AUG INFERENCE\n",
    "                ###############################################################\n",
    "\n",
    "                utils.check_batch_size(\n",
    "                    gen_inputs[\"input_ids\"].shape[0],\n",
    "                    generation_batch_size,\n",
    "                    len(question_dataloader.dataset),\n",
    "                )\n",
    "\n",
    "                query_aug_ids_batch, query_aug_scores_batch = (\n",
    "                    iterated_retrieval.decode(\n",
    "                        model=query_aug_model,\n",
    "                        batch=gen_inputs,\n",
    "                        tokenizer=tokenizer_bart,\n",
    "                        decoding_conf=decoding_conf_query_aug,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    utils.check_batch_size(\n",
    "                        query_aug_ids_batch.shape[0],\n",
    "                        generation_batch_size,\n",
    "                        len(question_dataloader.dataset),\n",
    "                    )\n",
    "                    utils.check_batch_size(\n",
    "                        query_aug_scores_batch.shape[0],\n",
    "                        generation_batch_size,\n",
    "                        len(question_dataloader.dataset),\n",
    "                    )\n",
    "\n",
    "                except RuntimeError as err:\n",
    "                    raise utils.add_to_err(err,\n",
    "                        f\"{query_aug_ids_batch.shape = }\\n\" +\n",
    "                        f\"{query_aug_scores_batch.shape = }\\n\" +\n",
    "                        f\"{gen_inputs['input_ids'].shape = }\\n\"\n",
    "                    )\n",
    "\n",
    "                query_aug_text_batch = []\n",
    "                for (\n",
    "                    question, query_aug_input, query_aug_ids_per_question\n",
    "                ) in more_itertools.zip_equal(\n",
    "                    questions_batch,\n",
    "                    gen_inputs[\"input_ids\"],\n",
    "                    query_aug_ids_batch,\n",
    "                ):\n",
    "                    texts_per_question = []\n",
    "                    for generation in query_aug_ids_per_question:\n",
    "                        gen = tokenizer_bart.decode(generation)\n",
    "                        cleaned = ir.clean_bart_decode(gen, tokenizer_bart)\n",
    "                        texts_per_question.append(cleaned)\n",
    "                    query_aug_text_batch.append(texts_per_question)\n",
    "\n",
    "                assert len(query_aug_text_all_loops) == loop_i + 1, (\n",
    "                    len(query_aug_text_all_loops), loop_i + 1\n",
    "                )\n",
    "                assert len(query_aug_score_all_loops) == loop_i + 1, (\n",
    "                    len(query_aug_score_all_loops), loop_i + 1\n",
    "                )\n",
    "\n",
    "                query_aug_text_batch = np.array(\n",
    "                    query_aug_text_batch, dtype=\"object\"\n",
    "                )\n",
    "\n",
    "                # Make sure that query_aug_text_batch are of the expected shape\n",
    "                utils.check_shape(\n",
    "                    query_aug_text_batch.shape,\n",
    "                    (\n",
    "                        query_aug_ids_batch.shape[0],\n",
    "                        decoding_conf_query_aug.num_return_sequences\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # The quantity of query aug to query score should be 1:1\n",
    "                utils.check_equal(\n",
    "                    query_aug_text_batch.shape[0],\n",
    "                    query_aug_scores_batch.shape[0],\n",
    "                )\n",
    "\n",
    "                # Accumulate the query augmentation text by loop\n",
    "                query_aug_text_all_loops[loop_i].extend(\n",
    "                    query_aug_text_batch\n",
    "                )\n",
    "\n",
    "                # Accumulate the query auggmentation generation score per loop\n",
    "                query_aug_score_all_loops[loop_i].extend(\n",
    "                    query_aug_scores_batch\n",
    "                )\n",
    "\n",
    "                ###############################################################\n",
    "                # READER INFERENCE\n",
    "                ###############################################################\n",
    "#                 reader_batch_ids, reader_batch_scores = (\n",
    "#                     iterated_retrieval.decode(\n",
    "#                         model=reader_model,\n",
    "#                         batch=gen_inputs,\n",
    "#                         tokenizer=tokenizer_bart,\n",
    "#                         decoding_conf=decoding_conf_reader,\n",
    "#                     )\n",
    "#                 )\n",
    "#                 reader_batch_ids = (\n",
    "#                     reader_batch_ids.squeeze(1)\n",
    "#                 )\n",
    "#                 # Decode the tokens of the batch\n",
    "#                 reader_text_batch = []\n",
    "#                 for (\n",
    "#                     question, query_aug_input, generations_ids, scores\n",
    "#                 ) in more_itertools.zip_equal(\n",
    "#                     questions_batch,\n",
    "#                     gen_inputs[\"input_ids\"],\n",
    "#                     reader_batch_ids,\n",
    "#                     reader_batch_scores,\n",
    "#                 ):\n",
    "#                     reader_text_batch.append(\n",
    "#                         ir.clean_bart_decode(\n",
    "#                             tokenizer_bart.decode(generations_ids),\n",
    "#                             tokenizer_bart\n",
    "#                         )\n",
    "#                     )\n",
    "\n",
    "                ###############################################################\n",
    "                # Deal with the generated text: reader inference\n",
    "                ###############################################################\n",
    "                # iterated_retrieval.write_generations(\n",
    "                #     reader_text_batch,\n",
    "                #     output_paths[\"reader_outs\"],\n",
    "                # )\n",
    "                iterated_retrieval.write_generations(\n",
    "                    query_aug_text_batch.tolist(),\n",
    "                    output_paths[\"q_aug_outs\"],\n",
    "                )\n",
    "                iterated_retrieval.write_generations(\n",
    "                    gen_inputs_text,\n",
    "                    output_paths[\"gen_inputs\"],\n",
    "                )\n",
    "\n",
    "\n",
    "inference(\n",
    "    all_passages=all_passages,\n",
    "    query_aug_model=query_aug_model.cuda(),\n",
    "    reader_model=reader_model.cuda(),\n",
    "    special_query_token=special_query_token,\n",
    "    retriever=retriever,\n",
    "    selection_technique=selection_technique,\n",
    "    question_dataloader=dataloader,\n",
    "    max_loop_n=args.max_loop_n,\n",
    "    query_aug_input_max_length=args.max_source_len,\n",
    "    decoding_conf_query_aug=args.decoding_conf_query_aug,\n",
    "    decoding_conf_reader=args.decoding_conf_reader,\n",
    "    n_docs=args.n_docs,\n",
    "    out_path=args.out_path,\n",
    "    retriever_batch_size=args.retriever_batch_size,\n",
    "    aug_method=args.aug_method,\n",
    "    final_num_contexts=args.final_num_contexts,\n",
    "    generation_batch_size=args.generation_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c70e6-e41a-463e-910d-8cfe70c026df",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(iterated_retrieval.question_generator(\n",
    "    dataloader, tokenizer_bart, \"asd\",\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
