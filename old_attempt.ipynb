{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d837c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Checking versions...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mChecking versions\u001b[0m\u001b[1;34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing torch...\n",
      "Done importing torch.\n",
      "Importing transformers...\n",
      "Done importing transformers.\n",
      "Importing tqdm...\n",
      "Done importing tqdm.\n",
      "Importing spacy...\n",
      "Done importing spacy.\n",
      "Importing hydra...\n",
      "Done importing hydra.\n",
      "Importing omegaconf...\n",
      "Done importing omegaconf.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">All version checks passed.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mAll version checks passed.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard library\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Third party\n",
    "import hydra\n",
    "import rich\n",
    "\n",
    "# First Party\n",
    "BASE_PATH = Path(\"/home/mila/g/gagnonju/DPR/\")\n",
    "CONF_PATH = BASE_PATH/\"conf\"\n",
    "\n",
    "os.chdir(BASE_PATH)\n",
    "import dense_retriever\n",
    "import jules_validate_dense_retriever\n",
    "from dense_retriever import *\n",
    "import common_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d991cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Resets the passage cache, which is the longest load time of the \n",
    "# script at 7 min\n",
    "###########################################################################\n",
    "all_passages = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919bdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = None\n",
    "retriever = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ceea68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Passed validation.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPassed validation.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ctx_datatsets'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'dpr_wiki'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ctx_sources'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'dpr_wiki'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dpr.data.retriever_data.CsvCtxSrc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'data.wikipedia_split.psgs_w100'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'id_prefix'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'datasets'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'nq_test'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dpr.data.retriever_data.CsvQASrc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'data.retriever.qas.nq-test'</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'nq_train'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dpr.data.retriever_data.CsvQASrc'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data.retriever.qas.nq-train'</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'nq_dev'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'dpr.data.retriever_data.CsvQASrc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data.retriever.qas.nq-dev'</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'trivia_test'</span>: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dpr.data.retriever_data.CsvQASrc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data.retriever.qas.trivia-test'</span><span style=\"font-weight: bold\">}</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'trivia_train'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dpr.data.retriever_data.CsvQASrc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'data.retriever.qas.trivia-train'</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'trivia_dev'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'dpr.data.retriever_data.CsvQASrc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data.retriever.qas.trivia-dev'</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'webq_test'</span>: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dpr.data.retriever_data.CsvQASrc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data.retriever.qas.webq-test'</span><span style=\"font-weight: bold\">}</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'curatedtrec_test'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dpr.data.retriever_data.CsvQASrc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'data.retriever.qas.curatedtrec-test'</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'device'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'distributed_world_size'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'do_lower_case'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'encoded_ctx_files'</span>: \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'/home/mila/g/gagnonju/DPR/downloads/data/wikipedia_split/psgs_w100.tsv'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'encoder'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'encoder_model_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'hf_bert'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pretrained_model_cfg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'bert-base-uncased'</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'pretrained_file'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'projection_dim'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sequence_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dropout'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'fix_ctx_encoder'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pretrained'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'encoder_path'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'fp16'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'fp16_opt_level'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'O1'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'global_loss_buf_sz'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150000</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'index_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/home/mila/g/gagnonju/DPR/dpr/downloads/indexes/single/nq/full'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'indexer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'flat'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'indexers'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'flat'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dpr.indexer.faiss_indexers.DenseFlatIndexer'</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hnsw'</span>:\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dpr.indexer.faiss_indexers.DenseHNSWFlatIndexer'</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hnsw_sq'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'_target_'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'dpr.indexer.faiss_indexers.DenseHNSWSQIndexer'</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'kilt_out_file'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'local_rank'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'match'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/home/mila/g/gagnonju/DPR/dpr/downloads/checkpoint/retriever/single/nq/ber</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">t-base-encoder.cp'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'n_docs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'n_gpu'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'no_cuda'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'out_file'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/home/mila/g/gagnonju/DPR/outputs/integrated_script_attempt.py'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'qa_dataset'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'nq_test'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'rpc_retriever_cfg_file'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'special_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'validate_as_tables'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'validation_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m128\u001b[0m,\n",
       "    \u001b[32m'ctx_datatsets'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'dpr_wiki'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'ctx_sources'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'dpr_wiki'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'dpr.data.retriever_data.CsvCtxSrc'\u001b[0m, \u001b[32m'file'\u001b[0m: \n",
       "\u001b[32m'data.wikipedia_split.psgs_w100'\u001b[0m, \u001b[32m'id_prefix'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'datasets'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'nq_test'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'dpr.data.retriever_data.CsvQASrc'\u001b[0m, \u001b[32m'file'\u001b[0m: \n",
       "\u001b[32m'data.retriever.qas.nq-test'\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'nq_train'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'dpr.data.retriever_data.CsvQASrc'\u001b[0m, \n",
       "\u001b[32m'file'\u001b[0m: \u001b[32m'data.retriever.qas.nq-train'\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'nq_dev'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \n",
       "\u001b[32m'dpr.data.retriever_data.CsvQASrc'\u001b[0m, \u001b[32m'file'\u001b[0m: \u001b[32m'data.retriever.qas.nq-dev'\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'trivia_test'\u001b[0m: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'dpr.data.retriever_data.CsvQASrc'\u001b[0m, \u001b[32m'file'\u001b[0m: \u001b[32m'data.retriever.qas.trivia-test'\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[32m'trivia_train'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'dpr.data.retriever_data.CsvQASrc'\u001b[0m, \u001b[32m'file'\u001b[0m: \n",
       "\u001b[32m'data.retriever.qas.trivia-train'\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'trivia_dev'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \n",
       "\u001b[32m'dpr.data.retriever_data.CsvQASrc'\u001b[0m, \u001b[32m'file'\u001b[0m: \u001b[32m'data.retriever.qas.trivia-dev'\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'webq_test'\u001b[0m: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'dpr.data.retriever_data.CsvQASrc'\u001b[0m, \u001b[32m'file'\u001b[0m: \u001b[32m'data.retriever.qas.webq-test'\u001b[0m\u001b[1m}\u001b[0m, \n",
       "\u001b[32m'curatedtrec_test'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'dpr.data.retriever_data.CsvQASrc'\u001b[0m, \u001b[32m'file'\u001b[0m: \n",
       "\u001b[32m'data.retriever.qas.curatedtrec-test'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'device'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'distributed_world_size'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'do_lower_case'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'encoded_ctx_files'\u001b[0m: \n",
       "\u001b[1m[\u001b[0m\u001b[32m'/home/mila/g/gagnonju/DPR/downloads/data/wikipedia_split/psgs_w100.tsv'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'encoder'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'encoder_model_type'\u001b[0m: \u001b[32m'hf_bert'\u001b[0m, \u001b[32m'pretrained_model_cfg'\u001b[0m: \u001b[32m'bert-base-uncased'\u001b[0m,\n",
       "\u001b[32m'pretrained_file'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'projection_dim'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'sequence_length'\u001b[0m: \u001b[1;36m256\u001b[0m, \u001b[32m'dropout'\u001b[0m: \u001b[1;36m0.1\u001b[0m, \n",
       "\u001b[32m'fix_ctx_encoder'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'pretrained'\u001b[0m: \u001b[3;92mTrue\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'encoder_path'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'fp16'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'fp16_opt_level'\u001b[0m: \u001b[32m'O1'\u001b[0m,\n",
       "    \u001b[32m'global_loss_buf_sz'\u001b[0m: \u001b[1;36m150000\u001b[0m,\n",
       "    \u001b[32m'index_path'\u001b[0m: \u001b[32m'/home/mila/g/gagnonju/DPR/dpr/downloads/indexes/single/nq/full'\u001b[0m,\n",
       "    \u001b[32m'indexer'\u001b[0m: \u001b[32m'flat'\u001b[0m,\n",
       "    \u001b[32m'indexers'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'flat'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'dpr.indexer.faiss_indexers.DenseFlatIndexer'\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'hnsw'\u001b[0m:\n",
       "\u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \u001b[32m'dpr.indexer.faiss_indexers.DenseHNSWFlatIndexer'\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'hnsw_sq'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'_target_'\u001b[0m: \n",
       "\u001b[32m'dpr.indexer.faiss_indexers.DenseHNSWSQIndexer'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'kilt_out_file'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'local_rank'\u001b[0m: \u001b[1;36m-1\u001b[0m,\n",
       "    \u001b[32m'match'\u001b[0m: \u001b[32m'string'\u001b[0m,\n",
       "    \u001b[32m'model_file'\u001b[0m: \u001b[32m'/home/mila/g/gagnonju/DPR/dpr/downloads/checkpoint/retriever/single/nq/ber\u001b[0m\n",
       "\u001b[32mt-base-encoder.cp'\u001b[0m,\n",
       "    \u001b[32m'n_docs'\u001b[0m: \u001b[1;36m100\u001b[0m,\n",
       "    \u001b[32m'n_gpu'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "    \u001b[32m'no_cuda'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'out_file'\u001b[0m: \u001b[32m'/home/mila/g/gagnonju/DPR/outputs/integrated_script_attempt.py'\u001b[0m,\n",
       "    \u001b[32m'qa_dataset'\u001b[0m: \u001b[32m'nq_test'\u001b[0m,\n",
       "    \u001b[32m'rpc_retriever_cfg_file'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'special_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'validate_as_tables'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'validation_workers'\u001b[0m: \u001b[1;36m15\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-08 01:04:25,047][root][INFO] - args.local_rank -1\n",
      "[2021-09-08 01:04:25,048][root][INFO] - WORLD_SIZE None\n",
      "[2021-09-08 01:04:25,049][root][INFO] - Initialized host cn-a002 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "[2021-09-08 01:04:25,050][root][INFO] - 16-bits training: False \n",
      "[2021-09-08 01:04:25,051][root][INFO] - CFG (after gpu  configuration):\n",
      "[2021-09-08 01:04:25,059][root][INFO] - indexers:\n",
      "  flat:\n",
      "    _target_: dpr.indexer.faiss_indexers.DenseFlatIndexer\n",
      "  hnsw:\n",
      "    _target_: dpr.indexer.faiss_indexers.DenseHNSWFlatIndexer\n",
      "  hnsw_sq:\n",
      "    _target_: dpr.indexer.faiss_indexers.DenseHNSWSQIndexer\n",
      "out_file: /home/mila/g/gagnonju/DPR/outputs/integrated_script_attempt.py\n",
      "validation_workers: 15\n",
      "n_gpu: 1\n",
      "qa_dataset: nq_test\n",
      "ctx_datatsets:\n",
      "- dpr_wiki\n",
      "encoded_ctx_files:\n",
      "- /home/mila/g/gagnonju/DPR/downloads/data/wikipedia_split/psgs_w100.tsv\n",
      "match: string\n",
      "n_docs: 100\n",
      "batch_size: 128\n",
      "do_lower_case: true\n",
      "encoder_path: null\n",
      "index_path: /home/mila/g/gagnonju/DPR/dpr/downloads/indexes/single/nq/full\n",
      "kilt_out_file: null\n",
      "model_file: /home/mila/g/gagnonju/DPR/dpr/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp\n",
      "validate_as_tables: false\n",
      "rpc_retriever_cfg_file: null\n",
      "indexer: flat\n",
      "special_tokens: null\n",
      "local_rank: -1\n",
      "global_loss_buf_sz: 150000\n",
      "device: cuda\n",
      "distributed_world_size: 1\n",
      "no_cuda: false\n",
      "fp16: false\n",
      "fp16_opt_level: O1\n",
      "encoder:\n",
      "  encoder_model_type: hf_bert\n",
      "  pretrained_model_cfg: bert-base-uncased\n",
      "  pretrained_file: null\n",
      "  projection_dim: 0\n",
      "  sequence_length: 256\n",
      "  dropout: 0.1\n",
      "  fix_ctx_encoder: false\n",
      "  pretrained: true\n",
      "datasets:\n",
      "  nq_test:\n",
      "    _target_: dpr.data.retriever_data.CsvQASrc\n",
      "    file: data.retriever.qas.nq-test\n",
      "  nq_train:\n",
      "    _target_: dpr.data.retriever_data.CsvQASrc\n",
      "    file: data.retriever.qas.nq-train\n",
      "  nq_dev:\n",
      "    _target_: dpr.data.retriever_data.CsvQASrc\n",
      "    file: data.retriever.qas.nq-dev\n",
      "  trivia_test:\n",
      "    _target_: dpr.data.retriever_data.CsvQASrc\n",
      "    file: data.retriever.qas.trivia-test\n",
      "  trivia_train:\n",
      "    _target_: dpr.data.retriever_data.CsvQASrc\n",
      "    file: data.retriever.qas.trivia-train\n",
      "  trivia_dev:\n",
      "    _target_: dpr.data.retriever_data.CsvQASrc\n",
      "    file: data.retriever.qas.trivia-dev\n",
      "  webq_test:\n",
      "    _target_: dpr.data.retriever_data.CsvQASrc\n",
      "    file: data.retriever.qas.webq-test\n",
      "  curatedtrec_test:\n",
      "    _target_: dpr.data.retriever_data.CsvQASrc\n",
      "    file: data.retriever.qas.curatedtrec-test\n",
      "ctx_sources:\n",
      "  dpr_wiki:\n",
      "    _target_: dpr.data.retriever_data.CsvCtxSrc\n",
      "    file: data.wikipedia_split.psgs_w100\n",
      "    id_prefix: ''\n",
      "\n",
      "[2021-09-08 01:04:25,060][root][INFO] - Reading saved model from /home/mila/g/gagnonju/DPR/dpr/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp\n",
      "[2021-09-08 01:04:26,253][root][INFO] - model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n",
      "[2021-09-08 01:04:26,496][transformers.configuration_utils][INFO] - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mila/g/gagnonju/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[2021-09-08 01:04:26,498][transformers.configuration_utils][INFO] - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2021-09-08 01:04:26,595][transformers.modeling_utils][INFO] - loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[2021-09-08 01:04:29,296][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "[2021-09-08 01:04:29,297][transformers.modeling_utils][INFO] - All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "[2021-09-08 01:04:29,437][transformers.configuration_utils][INFO] - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mila/g/gagnonju/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "[2021-09-08 01:04:29,438][transformers.configuration_utils][INFO] - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2021-09-08 01:04:29,495][transformers.modeling_utils][INFO] - loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[2021-09-08 01:04:32,199][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "[2021-09-08 01:04:32,200][transformers.modeling_utils][INFO] - All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "[2021-09-08 01:04:32,325][transformers.tokenization_utils_base][INFO] - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/mila/g/gagnonju/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[2021-09-08 01:04:32,355][root][INFO] - Selecting standard question encoder\n",
      "[2021-09-08 01:04:32,495][root][INFO] - Loading saved model state ...\n",
      "[2021-09-08 01:04:32,497][root][INFO] - Encoder state prefix question_model.\n",
      "[2021-09-08 01:04:32,605][root][INFO] - Encoder vector_size=768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Starting the thing.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mStarting the thing.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">dpr.data.retriever_data.CsvCtxSrc</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7f14e49e5970</span><span style=\"font-weight: bold\">&gt;]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mdpr.data.retriever_data.CsvCtxSrc\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7f14e49e5970\u001b[0m\u001b[1m>\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Second part.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mSecond part.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Using cached index.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mUsing cached index.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Using cached retriever.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mUsing cached retriever.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-08 01:04:32,633][root][INFO] - ctx_files_patterns: ['/home/mila/g/gagnonju/DPR/downloads/data/wikipedia_split/psgs_w100.tsv']\n",
      "[2021-09-08 01:04:32,635][root][INFO] - Embeddings files id prefixes: ['']\n",
      "[2021-09-08 01:04:32,636][root][INFO] - qa_dataset: nq_test\n",
      "[2021-09-08 01:04:32,651][dpr.data.download_data][INFO] - Requested resource from https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-test.qa.csv\n",
      "[2021-09-08 01:04:32,652][dpr.data.download_data][INFO] - Download root_dir /home/mila/g/gagnonju/DPR\n",
      "[2021-09-08 01:04:32,653][dpr.data.download_data][INFO] - File to be downloaded as /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/nq-test.csv\n",
      "[2021-09-08 01:04:32,654][dpr.data.download_data][INFO] - File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/nq-test.csv\n",
      "[2021-09-08 01:04:32,655][dpr.data.download_data][INFO] - Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
      "[2021-09-08 01:04:32,656][dpr.data.download_data][INFO] - File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/LICENSE\n",
      "[2021-09-08 01:04:32,656][dpr.data.download_data][INFO] - Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
      "[2021-09-08 01:04:32,657][dpr.data.download_data][INFO] - File already exist /home/mila/g/gagnonju/DPR/downloads/data/retriever/qas/README\n",
      "[2021-09-08 01:04:32,689][root][INFO] - Using custom representation token selector\n",
      "[2021-09-08 01:04:32,690][root][INFO] - id_prefixes per dataset: ['']\n",
      "[2021-09-08 01:04:32,691][root][INFO] - Using special token None\n",
      "[2021-09-08 01:04:48,289][root][INFO] - Encoded queries 3200\n",
      "[2021-09-08 01:04:50,299][root][INFO] - Total encoded queries tensor torch.Size([3610, 768])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">get_top_docs: Starting. Approx </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\"> min. Wednesday </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span><span style=\"font-weight: bold\"> September - </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">01:04:50</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mget_top_docs: Starting. Approx \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1m min. Wednesday \u001b[0m\u001b[1;36m08\u001b[0m\u001b[1m September - \u001b[0m\u001b[1;92m01:04:50\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-08 01:07:40,207][root][INFO] - index search time: 169.900069 sec.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">get_top_docs: Done.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mget_top_docs: Done.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">All done.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mAll done.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@hydra.main(config_path=CONF_PATH, config_name=\"dense_retriever\")\n",
    "def main(cfg):\n",
    "    ###########################################################################\n",
    "    # Complete and validate CFG\n",
    "    ###########################################################################\n",
    "    jules_validate_dense_retriever.validate(\n",
    "        {k: getattr(cfg, k) for k in dir(cfg)}, \n",
    "        dense_retriever.SCHEMA_PATH,\n",
    "    )\n",
    "    cfg = dense_retriever.setup_cfg_gpu(cfg)\n",
    "\n",
    "    assert cfg.out_file, cfg.out_file\n",
    "    assert Path(cfg.out_file).parent.exists(), cfg.out_file\n",
    "\n",
    "    logger.info(\"CFG (after gpu  configuration):\")\n",
    "    logger.info(\"%s\", OmegaConf.to_yaml(cfg))\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    # Prepare models\n",
    "    ###########################################################################\n",
    "#     saved_state = load_states_from_checkpoint(cfg.model_file)\n",
    "#     set_cfg_params_from_state(saved_state.encoder_params, cfg)\n",
    "\n",
    "#     tensorizer, encoder, _ = init_biencoder_components(\n",
    "#         cfg.encoder.encoder_model_type, cfg, inference_only=True\n",
    "#     )\n",
    "\n",
    "#     encoder_path = cfg.encoder_path\n",
    "#     if encoder_path:\n",
    "#         logger.info(\"Selecting encoder: %s\", encoder_path)\n",
    "#         encoder = getattr(encoder, encoder_path)\n",
    "#     else:\n",
    "#         logger.info(\"Selecting standard question encoder\")\n",
    "#         encoder = encoder.question_model\n",
    "\n",
    "#     encoder, _ = setup_for_distributed_mode(\n",
    "#         encoder, \n",
    "#         None, \n",
    "#         cfg.device, \n",
    "#         cfg.n_gpu, \n",
    "#         cfg.local_rank, \n",
    "#         cfg.fp16\n",
    "#     )\n",
    "#     encoder.eval()\n",
    "\n",
    "#     # load weights from the model file\n",
    "#     model_to_load = get_model_obj(encoder)\n",
    "#     logger.info(\"Loading saved model state ...\")\n",
    "\n",
    "#     encoder_prefix = (\n",
    "#         encoder_path if encoder_path else \"question_model\") + \".\"\n",
    "#     prefix_len = len(encoder_prefix)\n",
    "\n",
    "#     logger.info(\"Encoder state prefix %s\", encoder_prefix)\n",
    "#     question_encoder_state = {\n",
    "#         key[prefix_len:]: value\n",
    "#         for (key, value) in saved_state.model_dict.items()\n",
    "#         if key.startswith(encoder_prefix)\n",
    "#     }\n",
    "#     # TODO: long term HF state compatibility fix\n",
    "#     model_to_load.load_state_dict(question_encoder_state, strict=False)\n",
    "#     vector_size = model_to_load.get_out_size()\n",
    "#     logger.info(\"Encoder vector_size=%d\", vector_size)\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    # Prepare sources\n",
    "    ###########################################################################\n",
    "#     rich.print(\"[red bold]Starting the thing.\")\n",
    "#     id_prefixes = []\n",
    "#     ctx_sources = []\n",
    "#     for ctx_src in cfg.ctx_datatsets:\n",
    "#         ctx_src = hydra.utils.instantiate(cfg.ctx_sources[ctx_src])\n",
    "#         id_prefixes.append(ctx_src.id_prefix)\n",
    "#         ctx_sources.append(ctx_src)\n",
    "    \n",
    "#     rich.print(ctx_sources)\n",
    "#     rich.print(\"[red bold]Second part.\")\n",
    "    \n",
    "#     global all_passages\n",
    "#     if all_passages is None:\n",
    "#         all_passages = {}\n",
    "#         for ctx_src in ctx_sources:\n",
    "#             ctx_src.load_data_to(all_passages)\n",
    "#             rich.print(\"[green]Done loading passages.\")\n",
    "#         print(len(all_passages))\n",
    "\n",
    "    ###########################################################################\n",
    "    # Load Index & Prepare retriever\n",
    "    ###########################################################################\n",
    "\n",
    "#     index_path = cfg.index_path\n",
    "#     #------------\n",
    "#     ## Instantiate the index and create a retriever\n",
    "#     #------------\n",
    "#     global index\n",
    "#     global retriever\n",
    "#     if index is None or retriever is None:\n",
    "#         rich.print(f\"[bold blue]Loading index.\")\n",
    "#         index = hydra.utils.instantiate(cfg.indexers[cfg.indexer])\n",
    "#         logger.info(\"Index class %s \", type(index))\n",
    "#         index_buffer_sz = index.buffer_size\n",
    "#         index.init_index(vector_size)\n",
    "#         rich.print(f\"[bold blue]Done loading index.\")\n",
    "#         rich.print(f\"[bold blue]Loading retriever.\")\n",
    "#         retriever = LocalFaissRetriever(\n",
    "#             encoder, \n",
    "#             cfg.batch_size, \n",
    "#             tensorizer, \n",
    "#             index,\n",
    "#         )\n",
    "#         rich.print(f\"[bold blue]Loaded retriever.\")\n",
    "#         if index_path and index.index_exists(index_path):\n",
    "#             logger.info(\"Index path: %s\", index_path)\n",
    "#             retriever.index.deserialize(index_path)\n",
    "#         else:\n",
    "#             logger.info(\"Reading all passages data from files: %s\", input_paths)\n",
    "#             retriever.index_encoded_data(\n",
    "#                 input_paths, \n",
    "#                 index_buffer_sz, \n",
    "#                 path_id_prefixes=path_id_prefixes,\n",
    "#             )\n",
    "#             if index_path:\n",
    "#                 retriever.index.serialize(index_path)\n",
    "#     else:\n",
    "#         index_buffer_sz = index.buffer_size\n",
    "#         rich.print(f\"[bold green]Using cached index.\")\n",
    "#         rich.print(f\"[bold green]Using cached retriever.\")\n",
    "        \n",
    "\n",
    "    #------------\n",
    "    ## Index all passages\n",
    "    #------------\n",
    "#     ctx_files_patterns = cfg.encoded_ctx_files\n",
    "\n",
    "#     logger.info(\"ctx_files_patterns: %s\", ctx_files_patterns)\n",
    "#     if ctx_files_patterns:\n",
    "#         assert len(ctx_files_patterns) == len(\n",
    "#             id_prefixes\n",
    "#         ), \"ctx len={} pref leb={}\".format(\n",
    "#             len(ctx_files_patterns), \n",
    "#             len(id_prefixes),\n",
    "#         )\n",
    "#     else:\n",
    "#         assert (\n",
    "#             index_path\n",
    "#         ), \"Either encoded_ctx_files or index_path parameter should be set.\"\n",
    "\n",
    "#     input_paths = []\n",
    "#     path_id_prefixes = []\n",
    "#     for i, pattern in enumerate(ctx_files_patterns):\n",
    "#         pattern_files = glob.glob(pattern)\n",
    "#         pattern_id_prefix = id_prefixes[i]\n",
    "#         input_paths.extend(pattern_files)\n",
    "#         path_id_prefixes.extend([pattern_id_prefix] * len(pattern_files))\n",
    "\n",
    "#     logger.info(\"Embeddings files id prefixes: %s\", path_id_prefixes)\n",
    "    \n",
    "       \n",
    "    ###########################################################################\n",
    "    # Prepare questions and answers\n",
    "    ###########################################################################\n",
    "    questions = []\n",
    "    question_answers = []\n",
    "\n",
    "    if not cfg.qa_dataset:\n",
    "        logger.warning(\"Please specify qa_dataset to use\")\n",
    "        return\n",
    "\n",
    "    ds_key = cfg.qa_dataset\n",
    "    logger.info(\"qa_dataset: %s\", ds_key)\n",
    "\n",
    "    qa_src = hydra.utils.instantiate(cfg.datasets[ds_key])\n",
    "    qa_src.load_data()\n",
    "    assert not qa_src.selector, qa_src.selector\n",
    "    logger.info(\"Using custom representation token selector\")\n",
    "    retriever.selector = qa_src.selector\n",
    "\n",
    "    logger.info(\"id_prefixes per dataset: %s\", id_prefixes)\n",
    "\n",
    "    for ds_item in qa_src.data:\n",
    "        question, answers = ds_item.query, ds_item.answers\n",
    "        questions.append(question)\n",
    "        question_answers.append(answers)\n",
    "\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Get top k results.\n",
    "    ###########################################################################\n",
    "    logger.info(\n",
    "        \"Using special token %s\", \n",
    "        qa_src.special_query_token,\n",
    "    )\n",
    "    questions_tensor = retriever.generate_question_vectors(\n",
    "        questions, \n",
    "        query_token=qa_src.special_query_token,\n",
    "    )\n",
    "    \n",
    "    rich.print(\n",
    "        f\"[bold]get_top_docs: Starting. Approx 7 min. {timestamp()}\"\n",
    "    )\n",
    "    top_ids_and_scores = retriever.get_top_docs(\n",
    "        questions_tensor.numpy(), \n",
    "        cfg.n_docs,\n",
    "    )\n",
    "    rich.print(\"[bold green]get_top_docs: Done.\")\n",
    "\n",
    "    # we no longer need the index\n",
    "    retriever = None\n",
    "\n",
    "    if len(all_passages) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No passages data found. Please specify \"\n",
    "            \"ctx_file param properly.\"\n",
    "        )\n",
    "\n",
    "    rich.print(\"[green bold]All done.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "sys.argv = [\n",
    "    \"fake.py\", \n",
    "    \"out_file=/home/mila/g/gagnonju/DPR/outputs/integrated_script_attempt.py\",\n",
    "]\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaff0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCATENATION_TECHNIQUES = dict()\n",
    "MAX_LOOP_N = 15\n",
    "MODEL_PATH = None\n",
    "CONCATENATION_TECHNIQUE = None\n",
    "BATCH_SIZE_QUESTIONS = 10\n",
    "OUTPUT_FILES_ROOT = None\n",
    "\n",
    "make_batches = more_itertools.ichunked\n",
    "\n",
    "\n",
    "def write_contexts(\n",
    "    all_contexts, \n",
    "    context_ids, \n",
    "    output_files_root, \n",
    "    loop_i,\n",
    "):\n",
    "    with open(\n",
    "        output_files_root/f\"contexts_{loop_i}.txt\", \n",
    "        \"a\",\n",
    "    ) as f_out:\n",
    "        for context_id in context_ids:\n",
    "            f_out.write(all_contexts[context_id].strip() + \"\\n\")\n",
    "\n",
    "def write_generations(\n",
    "    generated_text,\n",
    "    output_files_root, \n",
    "    loop_i,\n",
    "):\n",
    "    with open(\n",
    "        output_files_root/f\"generated_{loop_i}.txt\", \n",
    "        \"a\",\n",
    "    ) as f_out:\n",
    "        text = \"\\n\".join((x.strip() for x in generated_text))\n",
    "        f_out.write(text)\n",
    "\n",
    "def inference(\n",
    "    all_contexts,\n",
    "    max_loop_n=MAX_LOOP_N, \n",
    "    model_path=MODEL_PATH, \n",
    "    concatenation_technique=CONCATENATION_TECHNIQUE,\n",
    "    output_files_root=OUTPUT_FILES_ROOT,\n",
    "):\n",
    "    \"\"\"\n",
    "    We currently do \n",
    "    \n",
    "    for batch_questions in questions:\n",
    "        for loop_i in range(max_loop_n)\n",
    "            encode_context_to_gen\n",
    "            (load generator to faster memory)\n",
    "            generate(batch)\n",
    "            decode_text_from_gen\n",
    "            \n",
    "            encode_text_to_retriever\n",
    "            (load retriever to faster memory)\n",
    "            retrieve(batch)\n",
    "            decode_text_from_retriever\n",
    "    \n",
    "    We could do\n",
    "    \n",
    "    for loop_i in range(max_loop_n)\n",
    "        # Parallelize as needed if helpful\n",
    "        # num_questions x num_beams to do\n",
    "        for batch_questions in zip(\n",
    "            retrieved_contexts\n",
    "        ):\n",
    "            encode_contexts_to_gen\n",
    "        \n",
    "        (imaginary barrier)\n",
    "        (load generator to faster memory)    \n",
    "        for batch_questions in questions:\n",
    "            top_beam_ids, top_beam_ppls = generate(batch)\n",
    "            \n",
    "        # Parallelize as needed if helpful\n",
    "        for batch_questions in questions:\n",
    "            decode_text_from_gen\n",
    "            \n",
    "        # Parallelize as needed if helpful\n",
    "        for batch_questions in questions:\n",
    "            encode_text_to_retriever\n",
    "\n",
    "        (imaginary barrier)\n",
    "        # Parallelize as needed if helpful\n",
    "        for batch_questions in questions:\n",
    "            concatenate_contexts\n",
    "\n",
    "        (imaginary barrier)\n",
    "        (load retriever to faster memory)\n",
    "        for batch_questions in questions:            \n",
    "            retrieve(batch)\n",
    "        \n",
    "        # Parallelize as needed if helpful\n",
    "        for batch_questions in questions:            \n",
    "            decode_text_from_retriever\n",
    "    \n",
    "    Much better for GPU memory locality,\n",
    "    worse for total memory use. Would maybe allow\n",
    "    for slightly larger batches.\n",
    "    \n",
    "    I think that the fact that we don't need the \n",
    "    GPU results right away makes it async and faster\n",
    "    \n",
    "    \"\"\"\n",
    "    output_files_root = pathlib.Path(output_files_root)\n",
    "    model = load_model(model_path)\n",
    "    concatenation_technique = CONCATENATION_TECHNIQUES[\n",
    "        concatenation_technique\n",
    "    ]\n",
    "    question_batches = make_batches(questions)\n",
    "    \n",
    "    context_ids_accum = [[] for _ in range(max_loop_n)]\n",
    "    generation_text_accum = [[] for _ in range(max_loop_n)]\n",
    "    \n",
    "    for question_batch in question_batches:\n",
    "        retrieval_query = question\n",
    "        all_generations = []\n",
    "        contexts = retrieve(retrieval_query)\n",
    "\n",
    "        for loop_i in range(max_loop_n):\n",
    "            top_beams_ids, top_beams_ppl = generate(\n",
    "                question, contexts,\n",
    "            )\n",
    "            \n",
    "            # Detokenize top_beams_ids.\n",
    "            # We do this here because we don't have the choice?\n",
    "            # Maybe we do though.\n",
    "            # batch_size x num_beams x (variable num_words)\n",
    "            top_beams_text = []\n",
    "            for unit in top_beams_ids:\n",
    "                top_beams_text.append([\n",
    "                    tokenizer_generator.decode(ids_beam) \n",
    "                    for ids_beam in unit\n",
    "                ])\n",
    "            \n",
    "            # We likely have to decode and re-encode here\n",
    "            retrieval_query = concatenation_technique(\n",
    "                question, \n",
    "                top_beams_text,\n",
    "                top_beams_ppl,\n",
    "                all_results, \n",
    "                loop_i,\n",
    "            )\n",
    "            \n",
    "            context_ids = retrieve(retrieval_query)            \n",
    "            generation_text_accum[loop_i].append(top_beams_text)\n",
    "            context_ids_accum[loop_i].append(contexts)\n",
    "    \n",
    "    \n",
    "    # The following are bad, \n",
    "    context_text_accum = [[] for _ in range(max_loop_n)]\n",
    "\n",
    "    \n",
    "    for i, batch in enumerate(context_ids_accum):\n",
    "        for id_ in batch:\n",
    "            context_text_accum[i].append(all_contexts[id_])\n",
    "    \n",
    "    # [Validate somewhere]\n",
    "    \n",
    "#             write_contexts(\n",
    "#                 all_contexts, \n",
    "#                 context_ids, \n",
    "#                 output_files_root, \n",
    "#                 loop_i,\n",
    "#             )\n",
    "            \n",
    "#             write_generations(\n",
    "#                 generated_text,\n",
    "#                 output_files_root,\n",
    "#                 loop_i,\n",
    "#             )\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
